---
title: "Project 1: Multivariate Non-Normal Distributions and Correlated Data"
author: "Shangzi Gao, Elena Jaurequi, Mirah Koota, Xiaoni Xu"
date: "March 3, 2025"
output: html_document
---

```{r load_packages, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(MASS)  
library(Matrix)
library(copula)
library(mvtnorm)
library(ggplot2) 
library(GGally)
library(DT)
library(patchwork)
library(kableExtra)
library(MVN)
library(robustbase)
library(knitr)
```



# Introduction

Traditionally, statistical models usually assume independence and normality. However, in many real-world scenarios, especially in epidemiology and medical research, data may have heavier tails (more extreme values) and inherent correlations among variables. For example, metabolic variables such as glucose and cholesterol, as well as cardiovascular measures like BMI and blood pressure, are naturally interrelated. Ignoring these features in a regression model can lead to biases and incorrect inferences.



## Data Generation Method

### Overview

Our goal is to simulate medical data that capture both heavy tails and correaltions among variables. We choose a multivariate t-distribution for its heavy-tailed character and use a t-copula as a predefined correlation structure on the marginal distributions. The four variables are:

- **Glucose** (mg/dL)

- **Cholesterol** (mg/dL)

- **BMI** (Body Mass Index)

- **Blood Pressure** (mmHg)


We begin by loading essential R packages such as `tidyverse`, `MASS`, `Matrix`, `copula`, and others that support simulation, matrix operations, and visualization. Key parameters are defined as follows:

Degrees of freedom (nu = 3)

Number of variables (p = 4) and sample size (n = 1000)

Mean vector (mu): Typical values for each health indicator



```{r set_parameters, message=FALSE}
set.seed(2025)

# Define parameters
nu <- 3  # Degrees of freedom (controls tail thickness)
p <- 4   # Number of variables
n <- 1000  # Number of samples

# Define mean vector
mu <- c(100,   # Mean Glucose Level (mg/dL)
        200,   # Mean Cholesterol Level (mg/dL)
        25,    # Mean BMI (Body Mass Index)
        120)   # Mean Blood Pressure (mmHg)
```


The metabolic block with Glucose and Cholesterol is assigned a correlation of 0.5, while the cardiovascular block with BMI and Blood Pressure is assigned a correlation of 0.4. These two correlation blocks are combined using a block-diagonal matrix, and we ensure that the resulting matrix is positive definite by applying the `nearPD` function.


```{r corr_structure}
# Define block structures
# Block-diagonal: high correlation within groups, no correlation across groups

# Glucose & Cholesterol (Metabolism)
block1 <- matrix(c(1, 0.5, 
                   0.5, 1), nrow=2, ncol=2) 
# BMI & Blood Pressure (Cardiovascular)
block2 <- matrix(c(1, 0.4, 
                   0.4, 1), nrow=2, ncol=2)  


# Combine blocks into a block-diagonal matrix
Sigma <- bdiag(block1, block2)

# Convert to a standard matrix format
# Ensure the correlation matrix is positive definite
Sigma <- as.matrix(nearPD(Sigma)$mat) 
colnames(Sigma) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
rownames(Sigma) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
df_sigma <- as.data.frame(Sigma)
knitr::kable(df_sigma, caption = "Correlation Matrix of Block-diagonal", digits = 3)
```

To capture the dependency structure while maintaining flexible marginal distributions, we construct a t-copula. This approach includes extracting the lower triangular portion of the correlation matrix to obtain the copula parameters, and then generating uniform samples on the unit interval. The t-copula is particularly useful here because it incorporates tail dependence. To create the marginal heavy-tailed distributions, we simulate a large number of values from independent t-distributions. These values are then scaled and shifted to align with the desired means and variances of each variable, and any values outside medically plausible bounds are filtered out. 

We then map the uniform samples obtained from the copula to the quantiles of the heavy-tailed t-distributions.We verify the correlation between variables by computing the empirical correlation matrix and displaying it using a formatted table. Additionally, we use a pairs plot with `ggpairs` to visualize the relationships among the variables, with density plots along the diagonal and scatter plots with correlation coefficients.




```{r t_copula_generation}
rho_vec <- Sigma[lower.tri(Sigma)]  

# Define the t-Copula
copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

# Generate Copula-Based Uniform Samples
U_t <- rCopula(1000, copula_t) 

# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

# Generate Independent Heavy-Tailed t-Distributions
glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]

# Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]


# Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
data_copula_t <- data.frame(
  Glucose = quantile(glucose, probs = U_t[,1]),
  Cholesterol = quantile(cholesterol, probs = U_t[,2]),
  BMI = quantile(BMI, probs = U_t[,3]),
  BloodPressure = quantile(blood_pressure, probs = U_t[,4])
)

# Generate correlation matrix
corr_matrix <- cor(data_copula_t) %>% as.data.frame()
knitr::kable(corr_matrix, caption = "Correlation Matrix of t_copula", digits = 3)
```

 

```{r visualize_t_copula}
ggpairs(data_copula_t, 
        title = "t-Copula: Multivariate t-Distribution",
        lower = list(continuous = "points"),
        diag = list(continuous = "densityDiag"),
        upper = list(continuous = "cor")) +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14))
```

This figure shows a pairs plot of the four simulated variables—-Glucose, Cholesterol, BMI, and Blood Pressure that are generated from a t-copula with a specified block-diagonal correlation structure. Each panel along the diagonal shows the marginal distribution of a single variable. Glucose and Cholesterol peak around 100 mg/dL and 200 mg/dL respectively. Similarly, BMI and Blood Pressure center near 25 and 120 mmHg, and both pairs show relatively heavy tails. In the upper triangle of the figure, the panels report pairwise correlation coefficients, along with significance levels. The scatter plots in the lower triangle show that plots with higher correlations have elongated data clumps indicating linear relationships, and those with near-zero correlations have a more diffuse pattern. The figure demonstrates that our data generation procedure successfully captured both the heavy-tailed character of each marginal distribution and the specified block-diagonal correlation structure.   

# 2. Implement your data generation method

## Generate Data

The following code generates a dataset using a multivariate t-Copula approach to model dependencies among physiological variables (Glucose, Cholesterol, BMI, and Blood Pressure). It constructs a block-diagonal correlation matrix, samples from a t-Copula, and maps the uniform samples to heavy-tailed distributions.   
The resulting dataset, consisting of 1,000 observations with controlled correlation and variability, is saved as "generated_t_copula_data.csv" for further analysis.  

```{r generate_data}
# Function for generate multivariate t-distribution data 
generate_t_copula_data <- function(n = n, nu = nu) {
  
  # Define block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 
                     0.5, 1), nrow=2, ncol=2)  # Metabolic Block (Glucose & Cholesterol)
  block2 <- matrix(c(1, 0.4, 
                     0.4, 1), nrow=2, ncol=2)  # Cardiovascular Block (BMI & Blood Pressure)

  # Combine blocks into a correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure it is positive definite

  # Extract lower triangle correlations for t-Copula
  rho_vec <- Sigma[lower.tri(Sigma)]  

  # Define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

  # Generate Copula-Based Uniform Samples
  U_t <- rCopula(n, copula_t) 

  # Define upper and lower bounds for each variable
  glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
  glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL
  
  cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
  cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL
  
  BMI_min <- 15    # minimum plausible BMI
  BMI_max <- 40    # maximum plausible BMI
  
  blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
  blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg
  
  # Generate Independent Heavy-Tailed t-Distributions
  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
  glucose <- glucose_raw[glucose_raw >= glucose_min & 
                           glucose_raw <= glucose_max]
  cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min 
                                 & cholesterol_raw <= cholesterol_max]
  BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
  blood_pressure <- blood_pressure_raw[blood_pressure_raw 
                                       >= blood_pressure_min & 
                                         blood_pressure_raw <= 
                                         blood_pressure_max]
  
  
  # Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
  data_copula_t <- data.frame(
    Glucose = quantile(glucose, probs = U_t[,1]),
    Cholesterol = quantile(cholesterol, probs = U_t[,2]),
    BMI = quantile(BMI, probs = U_t[,3]),
    BloodPressure = quantile(blood_pressure, probs = U_t[,4])
  )

  return(data_copula_t)
}

# Generate a dataset with 1000 samples and df = 5
generated_data <- generate_t_copula_data(n = n, nu = nu)

# Save data as CSV
write.csv(generated_data, "generated_t_copula_data.csv", row.names = FALSE)
```

## Evaluate and Compare with Target Distribution 

Then we create histograms for Glucose, Cholesterol, BMI, and Blood Pressure, overlaid with fitted t-distribution curves. The goal is to visually assess how well a t-distribution matches the generated data.  

```{r visualization_hist_fit}
plot_distribution <- function(data, variable_name, df=5) {
  x_min <- min(data[[variable_name]])
  x_max <- max(data[[variable_name]])
  mean_val <- mean(data[[variable_name]])
  sd_val <- sd(data[[variable_name]])
  
  ggplot(data, aes(x = .data[[variable_name]])) +  
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +  
    stat_function(fun = function(x) dt((x - mean_val) / sd_val, df = df) / sd_val, 
                  col = "red", linewidth = .8) +  
    labs(title = paste("Histogram & t-Dist Fit for", variable_name), 
         x = variable_name, y = "Density") +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0))
}

p1 <- plot_distribution(generated_data, "Glucose")
p2 <- plot_distribution(generated_data, "Cholesterol")
p3 <- plot_distribution(generated_data, "BMI")
p4 <- plot_distribution(generated_data, "BloodPressure")

(p1 + p2) / (p3 + p4)
```

This visualization compares the empirical distributions of the generated data (blue histograms) with the fitted t-distributions (red curves). The results show that the t-distribution captures both the central tendency and heavy tails of the data.  

However, limiting the data range may have resulted in the exclusion of some extreme tail values, which could impact the full representation of heavy-tailed behavior.  

## Evaluate and Compare with Target Correlation    

We use `cov.trob()` function estimates the location (mean) assuming a heavy-tailed multivariate t-distribution.  

```{r}
fit_t <- cov.trob(generated_data)
print(fit_t$center)  # Estimated mean
```

The estimated mean values of the generated dataset are very close to the intended means(100, 200, 25, 200) used in the data generation process. This suggests that the location parameter of the multivariate t-distribution is well preserved.  

Now we'll use RMSE to check how well the correlation structure of the generated data matches the target correlation structure from the t-Copula. This will give us a clear measure of how much the actual correlations deviate from what we intended, helping us assess how well the data generation process preserved the dependencies.  

```{r U_t_cor_matrix}
df_U_t <- as.data.frame(U_t)
colnames(df_U_t) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
cor_U_t <- cor(df_U_t)
knitr::kable(cor_U_t, caption = "Correlation Matrix of Transformed t-Copula Samples", digits = 3)
```

```{r}
cor_generated_data <- cor(generated_data)
colnames(cor_generated_data) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
knitr::kable(cor_generated_data, caption = "Correlation Matrix of Generated Data", digits = 3)
```

The RMSE formula: $\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (r_i - \hat{r}_i)^2}$  
- $r_i$represents the elements of the target correlation matrix (from the t-Copula).    
- $\hat{r}_i$ represents the elements of the correlation matrix of the generated data.    

```{r comput_rmse}
rmse <- function(mat1, mat2) {
  sqrt(mean((mat1 - mat2)^2))
}
rmse_value <- rmse(cor_U_t, cor_generated_data)
```

The RMSE between the target correlation structure (from the t-Copula) and the empirical correlation matrix (from the generated data) is `r round(rmse_value, 4)`. This shows that the generated data closely matches the target correlation structure, with only minor deviations.   

These small differences may result from sampling variability, truncation, or the heavy-tailed nature of the t-distribution, but overall, the t-Copula approach effectively preserves the intended dependencies.  

# 3. Simulation Study 1

*"Fit a regression model to the simulated data while assuming that data are i.i.d and normally distributed. Conduct a simulation study that varies sample sizes, correlation levels, or degree of non-normality to assess bias in parameter estimates, type I error in hypothesis testing and coverage in conference intervals. Demonstrate how inference changes as the severity of assumption deviation increases (more skew, heavier tail, stronger correlation."*

* Interpretation of instructions: *"Vary sample size, correlation level, or degree of non-normality."* In the code below, we choose from two of these options: to vary 1. sample size and 2. heaviness of tails (i.e. deviation from normality)
* Interpretation of instructions: *"Assess bias in parameter estimates, type I error in hypothesis testing, and coverage in CIs."* In the code below, we do the following for each simulation setting:
  1) Assess bias in parameter estimates by characterizing the mean estimated regression coefficients across n=1000 simulations, and by characterizing the empirical 95% CI of those estimates.
  2) Calculate the percentage of regression-fit 95% CIs [calculated under the incorrect assumption of normality] from the n=1000 simulationswhich included the true, theoretical value for a given regression coefficient. This gives us the 1 - [the empirical risk of a 'Type I error'], where Type I error can be defined as the probability that the regression-fit  95% CI will exclude the true value of the regression coefficient. Specifically: 1 - P(Type I error) = P(true value ~in~ 95% CI), where P(true value ~in~ 95% CI) is the proportion of the n=1000 simulations for which the true value *was* in the regression-fit 95% CI.
  
## Simulation studies and procedures
We choose as our regression model Multiple Linear Regression. In particular, we perform the following regression:
  
  \[ Y = \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 + \beta_3 * X_3 \]

where Y is Glucose, our outcome; X1 is Cholesterol, X2 is BMI, and X3 is Blood Pressure (BP).


Multiple linear regression (MLR) assumes a linear relationship between Y and all X, normally distributed residuals, and no multicollinearity in the set of covariates. The last two of these assumptions are both violated by correlated data we have generated. With heavy-tailed t-distributions for the independent and dependent variables, the model residuals are likely to not be non-normally distributed; and, with the correlation structure we induced, BMI and BP (two of our covariates) are highly correlated. We expect to see biased estimates and inference in using this regression structure to model our data, given that the data violate model assumptions.

We examined the effect of sample size (of the generated data samples) and heavy tails (of the underlying outcome and covariate distributions) on bias and inference (using MLR). 

a) First, we varied generated sample size to examine the effect of sample size on bias & inference. 
b) Second, we varied the degrees of freedom to the t-distribution to examine the effect of heavy tails (i.e. deviation from normality) on bias and inference.

### a. The effect of (generated) sample size on bias and inference, using a model which assumes *iid* and normally distributed data

- Simulation procedure: 
  - Vary through 10 sample sizes, across differing orders of magnitude
    - For each sample size, extract...
      - Estimated regression coefficient for each covariate (Cholesterol, BMI, BP) 
      - Whether or not the "normal" 95% CI included the true theoretical value for the regression coefficient for each covariate (Cholesterol, BMI, BP)
  - For each sample size setting: Simulate (repeat the above) n = 1000 times
    - For each covariate, we will summarize...
      - The mean estimated regression coefficient (across runs) & empirical 95% CI – *This gives us insight into the bias and variability of estimates provided by MLRs fit to the simulated samples of each sample size.*
      - The percentage of "normal" 95% CIs (% of runs) which included the true value – *This gives us insight into the probability of a Type I error, where the confidence interval generated by the the multiple linear regression does not include the true theoretical value. We assess how this changes with the sample size of the simulated samples.*
  
```{r simstudy1_1, include=FALSE}
# Fit (multivariable) linear regression -- assumes iid and errors/residuals ~ N(), which can be violated with skewed/heavy-tailed distributions of explanatory or outcome variables (which is the case for all variables we have generated)

# clean environment
rm(list=setdiff(ls(), c("generate_t_copula_data","rho_vec", "p", "mu"))) # all I need for simulation study 1

# Get theoretical regression coefficients:

# ```{r generate_data} -- due to the work in this chunk, the standard deviation was rescaled (i.e. not dependent purely on t distribution df) -- numbers pulled out below:
sds_tdists <- c("glucose" = 15, "cholesterol" = 25, "BMI" = 3, "blood_pressure" = 10)
# regression coefficient = r*(sd_y/sd_x)
# for the 3 regression coefficients: Glucose-Cholesterol, Glucose-BMI, Glucose-BP: 
# Glucose-Cholesterol: 0.5 * 15/25
# Glucose-BMI, Glucose-BP: 0 (r=0)
target_reg_coeffs <- c("glucose_chol" = 0.5 * 15/25, "glucose_bmi" = 0, "glucose_bp" = 0)

# Now: choose one of the variables as the output, the other three as covariates. Choice is arbitrary, since this
# simulation study is a proof of concept/demonstration.

# Choose Glucose as Y; Cholesterol, BMI, Blood Pressure as X1, X2, X3

# show that the linreg can run:
gendata_ss1_test <- generate_t_copula_data(n = 100, nu = 3) # 3 df--lower df means heavier tails, but does not determine sd, due to the scaling performed above
lm(gendata_ss1_test$Glucose ~ gendata_ss1_test$Cholesterol + gendata_ss1_test$BMI + gendata_ss1_test$BloodPressure)
# It does a great job with the true beta1!...not so great with the covariates that should have been null
rm(gendata_ss1_test)
```


```{r}
# Vary sample size
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_sampsize <- ceiling(10^seq(1,4.5,length.out=10))

# generate samples & store correlations
tdist_df = 3 # arbitrary choice -- degree of heavy tails/variation from normality (t dist df) fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyss_storebetas <- function(fxn_gen, samp_sizes, tdist_degfree){ # give a fxn to generate data, list of sample sizes to vary through, & degrees of freedom("df") of t dists (fixed in this simulation)
  
  # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (nn in samp_sizes){
    
    gendata <- fxn_gen(n = nn, nu = tdist_df)
    # run regression
    lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
    
    # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
    # CIs
    lm_outputs <- as.data.frame(confint(lm_nn))
    colnames(lm_outputs) <- c("lower_95", "upper_95")
    lm_outputs <- lm_outputs[2:4,]
    rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    # point values
    lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
    # marker for loop [generated sample size] & true values & pairname
    lm_outputs$gensampsize <- nn
    lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
    lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    
    # store in list
    lm_out[[paste0("sampsize", nn)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
}
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varysampsize <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyss_storebetas(fxn_gen = generate_t_copula_data, 
                                              samp_sizes = vary_sampsize, tdist_degfree = tdist_df)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r, include=FALSE}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim <- run_simstudy1_varysampsize(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim, "simstudy1_1000sim.csv", row.names = FALSE)
```

```{r, include=FALSE}
# read in csv
library(here) # Rproject
simstudy1_1000sim <- read.csv(here("./simstudy_1/simstudy1_1000sim.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim$trueval_inCI <- (simstudy1_1000sim$true_val <= simstudy1_1000sim$upper_95) & (simstudy1_1000sim$true_val >= simstudy1_1000sim$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications

simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r, echo =FALSE}
# Plotting

# facet labels
# facet

coeff_names <- c(
  'Gluc_BMI'="BMI",
  'Gluc_BP'="BP",
  'Gluc_Chol'="Cholesterol"
  )

# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)
ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("MLR coefficient estimates vs. Sample size") + 
  labs(x = "Generated sample size",
       y = "Mean reg. coeff. estimate across runs (empirical 95% CI)",
       subtitle = "n = 1000 runs. Green line indicates true value.",
       caption = "With increasing sample size of generated data, variation in MLR coefficient estimates
       decreased, and we observed the bias more clearly (non-consistent estimator.)")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  ggtitle("1 - P(Type I Error) vs. Sample size, in the presence of bias") + 
  labs(x = "Generated sample size",
       y = "Proportion of runs where MLR 95% CI included true value",
       subtitle = "n = 1000 runs",
       caption = "Increasing sample size of generated data led to more incorrect inference; the model became 
       more confident in its estimates, while the mismatch between our model assumptions and data 
       generating mechanism (bias) remained constant.")

# note: these results SEEM counterintuitive, but it makes sense!! with the sample size increasing, we *think* we are more precise--but as the CI shrinks and shrinks, it is less likely to include the true value (since we are using an inappropriate model) EVEN AS the model gets more and more confident in its estimate! FASCINATING
```


_Interpretation:_

As sample sizes increased, the variation in the regression coefficient estimates estimated by the MLR decreased. This makes sense; with larger sample sizes, there would be less variability in the shape of the distributions generated with our data-generating mechanism (LLN), and so the relationship between those data would be more and more similar, leading to more consistent regression coefficient estimates between simulations. While the coefficients for BMI and BP were centered around their true values (indicating a lack of bias in these estimates), the bias for the estimate of the cholesterol coefficient grew more pronounced as sample size increased. Potential intuition for this could be that BMI and BP were, as specified in our true correlation matrix, completely uncorrelated with the outcome (Glucose). In contrast, the coefficient estimate for Cholesterol was meant to estimate a true value of 0.3, but was likely converging to a biased estimate as sample size increased due to non-normal residuals in the multiple linear regression (MLR).

As sample sizes increased, the probability of the MLR 95% CIs *including* the true regression coefficient values decreased, indicating that the probability of those CIs *excluding* the true regression coefficient values (i.e. the probability of a Type I error) increased. This trend was clear across all three covariates. The most likely explanation is that, as sample size increased, the MLR model became more confident in its estimates and produced narrower 95% CIs. However, since the model is misspecified (its assumptions are not met with the data provided), its results are biased, and the narrower 95% CIs are converging around a biased value (the MLR regression coefficient is a non-consistent estimator of the true Glucose-covariate relationships.) 

Note that this also explains why the probability of a Type I error is  higher for the Cholesterol regression coefficient (increases to >75% with n>10,000 samples) than it is for BP and BMI regression coefficients. The first plot (MLR coefficient estimates vs. samples size) showed us how  the MLR model produced especially biased estimates for the Cholesterol regression coefficient; therefore, as the 95% CI shrank towards a biased value with increasing sample size, the risk of a Type I error grew faster for this regression coefficient in particular. 


### b. The effect of outcome & covariate variable distribution heavy tails on bias and inference, using a model which assumes *iid* and normally distributed data

- Simulation procedure: 
  - Vary through 7 degrees of freedom settings, applied to all four t-distributions being generated/modeled
    - For each sample size, extract...
      - Estimated regression coefficient for each covariate (Cholesterol, BMI, BP) 
      - Whether or not the "normal" 95% CI included the true theoretical value for the regression coefficient for each covariate (Cholesterol, BMI, BP)
  - For each df setting: Simulate (repeat the above) 1000 times
    - For each covariate, we will summarize...
      - The mean estimated regression coefficient (across runs) & empirical 95% CI – *This gives us insight into the bias and variability of estimates for models with data from each degrees-of-freedom setting for the t-distributions, which dictates heaviness of tails & therefore how much the t-distribution approaches a normal distribution.*
      - The percentage of "normal" 95% CIs (% of runs) which included the true value – *This gives us insight into the probability of a Type I error, where the confidence interval generated by the the multiple linear regression does not include the true theoretical value. We asses how this changes with each degrees-of-freedom setting for the t-distributions, which dictates heavines of tails & therefore how much the t-distribution approaches a normal distribution.*
  
```{r}
# Vary heaviness of tails
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_heavytail <- 3^seq(1,7)

# generate samples & store correlations
sampsize_gen = 500 # arbitrary choice -- generated sample size fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyht_storebetas <- function(fxn_gen, samp_size, tdist_degfree){ # give a fxn to generate data, list of degrees of freedom to iterate through, & sample size for generated sample (fixed in this simulation)
  
  # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (degfree in tdist_degfree){
    
    gendata <- fxn_gen(n = samp_size, nu = degfree)
    # run regression
    lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
    
    # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
    # CIs
    lm_outputs <- as.data.frame(confint(lm_nn))
    colnames(lm_outputs) <- c("lower_95", "upper_95")
    lm_outputs <- lm_outputs[2:4,]
    rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    # point values
    lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
    # marker for loop [degfreeness] & true values & pairname
    lm_outputs$tdist_df <- degfree
    lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
    lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    
    # store in list
    lm_out[[paste0("tdistdf", degfree)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
}
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varyht <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyht_storebetas(fxn_gen = generate_t_copula_data, 
                                                samp_size = sampsize_gen, tdist_degfree = vary_heavytail)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r, include = FALSE}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim_1 <- run_simstudy1_varyht(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim_1, "simstudy1_1000sim_1.csv", row.names = FALSE)
```

```{r, include=FALSE}
# read in csv
simstudy1_1000sim_1 <- read.csv(here("./simstudy_1/simstudy1_1000sim_1.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim_1$trueval_inCI <- (simstudy1_1000sim_1$true_val <= simstudy1_1000sim_1$upper_95) &
  (simstudy1_1000sim_1$true_val >= simstudy1_1000sim_1$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications


simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r, echo=FALSE}
# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)

ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_1_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("MLR coefficient estimates vs. t-dist degrees of freedom ") + 
    labs(x = "df of t-distributions (for Glucose, BMI, BP, & Cholesterol)",
         y = "Mean reg. coeff. estimate across runs (empirical 95% CI)",
         subtitle = "n = 1000 runs. Green line indicates true value.",
         caption = "With increasing df, the t-distributions of the outcome and covariates approach normality, 
         and bias in the MLR estimates decreases.")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  ggtitle("1 - P(Type I Error) vs. t-dist degrees of freedom ") + 
  labs(x = "df of t-distributions (for Glucose, BMI, BP, & Cholesterol)",
       y = "Proportion of runs where MLR 95% CIs included true value",
       subtitle = "n = 1000 runs.",
       caption = "With higher df, t-distribution approaches normal distribution. Increasing the degrees of freedom 
       the data-generating mechanisms led to less incorrect inference; as the outcome and covariate distributions
       approached normality, the model's assumptions were less and less violated (less bias).")

```


_Interpretation:_

As the degrees of freedom to the t-distributions increased, the bias of the model-estimated regression coefficients decreased. This is as we would expect. The t-distribution approaches the normal distribution as the degrees of freedom approaches infinity; therefore, as we increased the degrees of freedom, we lessened the violation of the MLR that we had introduced by using heavy-tailed, non-normal distributions in the data. (Note: we see that precision was not affected by the reduction in the heaviness of the distribution tails. This is as expected, given that sample size was fixed in these simulations.)

As the degrees of freedom increased (i.e. less deviation from normality), the probability of the MLR 95% CIs *including* the true regression coefficient values increased, indicating that the probability of those CIs *excluding* the true regression coefficient values (i.e. the probability of a Type I error) decreased. This trend was clear for all three covariates. In fact, all three covariates converged around a 95% probability of including the true coefficient value, meaning a 5% probability of a Type I error. This indicates that, as the t-distributions approached normality, the 95% CIs generated by MLR (under the assumption of normality) converged to being _true_ 95% CIs.


# 2. Simulation Study 2


### Simulation studies and procedures

  For this simulation, robust regression was used to mitigate the influence of outliers and heavy-tailed distributions. It is useful when ordinary least squares assumptions don't hold up. Specifically, MM-estimation was performed because this method is useful when handling correlated and non-normal data. The model works by using two steps. The first step is an initial robust estimation with $\beta$ of high resistance to outliers, and the second step involves refining the estimate using a robust lost function. This form of regression downweigns the influence of outliers and works well with hravy-tailrf data. MM-estimation robust regressing has a high breakdown point compared to other regression models, meaning a high fraction of outliers for which a model won't break down.
\newline

The model formula for MM-Robust regression is:
$$
Y = \beta_0 + \beta_1*X_1 + \beta_2 * X_2 + \beta_3 * X_3 + \epsilon
$$
Where Y is glucose, and X are the predictors cholesterol, BMI, and blood Pressure. $\beta$ is the coefficent for each predictor and $\epsilon$ is the error term. 
\newline
The formula for the robust estimator is:
$$
\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n} \rho \left( \frac{y_i - x_i^\top \beta}{\hat{\sigma}} \right)
$$

Where:

$y_i$ : dependent variable
$x_i$ : predictor variable
$\beta$ : regression coefficients
$\sigma$ : robust estimate of residual standard deviation
$\rho$: Robust loss function

  \newline
Data generation was done via by creating a function from the t-Copula code used in parts 1 and 2 of this study. To summarize, this function used a block-diagonal correlation structure, created a heavy tailed distribution, and sets realistic bounds for the predictors and outcomes. Additionally, the data was centered around the true mean for each simulation by subtracting the true mean for each predictor from each value. This centers the data around 0 and decreases the variance of the intercept value in the modesl. The simulation scenarios are outlined in the table below.
\newline 
Table: Simulation Scenarios 

| Scenario       | n  | Degrees of Freedom | Correlation 1  | Correlation 2 | Extra            | 
|----------------|----|--------------------|----------------|---------------|------------------|
|Original        |100 |5                   | 0.5            | 0.4           | N/A              |
|Larger sample n |500 |5                   | 0.5            | 0.4           | N/A              |
|High Correlation|100 |5                   | 0.8            | 0.7           | N/A              |
|Low Correlation |100 |5                   | 0.2            | 0.1           | N/A              |
|High DoF        |100 |6                   | 0.5            | 0.4           | N/A              |
|Low DoF         |100 |4                   | 0.5            | 0.4           | N/A              |
|BMI Interaction |100 |5                   | 0.5            | 0.4           |Linear BMI/Glucose|

\newline
The packed used for MM-estimator robust regression was "robustbase" and the function used was "lmrob". Models were assessed by variance, bias, and R Squared value. Variance and R squared are reported in the output of lmrob, and bias was found by calculating the centered expected value of glucose using each model. Since the generated data was centered around zero, the expected value of glucose calculated from the model represents the actual expected value of glucose minus the true mean. 

```{r, echo=FALSE}
#taken from parts 1 and 2 and turned into a function so I can change parapeters in the simulation

#function to generate correlated non-normal data using a t-Copula (used part 2)
generate_t_copula_data <- function(n, nu, rho_mod = NULL) {
  #block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)  #metabolic block
  block2 <- matrix(c(1, 0.4, 0.4, 1), 2, 2)  #cardiovascular block
  
  #modify correlation for later
  if (!is.null(rho_mod)) {
    block1[1,2] <- block1[2,1] <- rho_mod[1]
    block2[1,2] <- block2[2,1] <- rho_mod[2]
  }
  
  #correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)
  rho_vec <- Sigma[lower.tri(Sigma)]  
  
  #define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=4, df=nu, dispstr="un")
  U_t <- rCopula(n, copula_t)  
  
  #generate independent heavy-tailed distributions
  mu <- c(100, 200, 25, 120) #the means
  
# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  bmi_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
bmi <- bmi_raw[bmi_raw >= BMI_min & bmi_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]

  #map Copula Uniform Samples Back to Heavy-Tailed Distributions
  data_copula_t <- data.frame(
    glucose = quantile(glucose, probs = U_t[,1]),
    cholesterol = quantile(cholesterol, probs = U_t[,2]),
    bmi = quantile(bmi, probs = U_t[,3]),
    blood_pressure = quantile(blood_pressure, probs = U_t[,4])
  )
  return(data_copula_t)
}

```

```{r,echo=FALSE, results='hide'}
set.seed(2025)

original_data <- generate_t_copula_data(n = 100, nu = 5)
original_data <- original_data %>%
  mutate(across(everything(), ~ . - mean(.)))#centering the data. this prevents the intercept variance from over affecting the data 
original_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = original_data)
summary(original_model) #robust regression using MM
```

```{r, echo=FALSE, results='hide'}
set.seed(2025)

large_data <- generate_t_copula_data(n = 500, nu = 5)
large_data <- large_data %>%
  mutate(across(everything(), ~ . - mean(.)))
large_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = large_data)
summary(large_model)
```

```{r, echo=FALSE, results='hide'}
set.seed(2025)
high_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.8, 0.7))
high_corr_data <- high_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_corr_data)
summary(high_corr_model)

low_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.2, 0.1))
low_corr_data <- low_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_corr_data)
summary(low_corr_model)
```

```{r, echo=FALSE, results='hide'}
low_df_data <- generate_t_copula_data(n = 100, nu = 4)
low_df_data <-low_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_df_data)
summary(low_df_model)

high_df_data <- generate_t_copula_data(n = 100, nu = 6)
high_df_data <- high_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_df_data)
summary(high_df_model)

```

```{r, echo=FALSE, results='hide'}
set.seed(2025)
bmi_data <- generate_t_copula_data(n = 100, nu = 5)

#Add a strong linear effect of bmi on glucose
bmi_data$glucose <- bmi_data$glucose + 2 * bmi_data$bmi  # Adjust the coefficient (0.5) as needed

#center
bmi_data <- bmi_data %>% mutate(across(everything(), ~ . - mean(.)))

#robust regression model
bmi_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = bmi_data)
summary(bmi_model)
```
## Model Results

The tables in this section include results that are important to understanding how each model fits the generated data and how changing simulation scenarios effects the model generation. For the full model output, please see the appendix. 

\newline
The table below show the variance of the intercepts and each predictor for all of the models. The last column on the right lists the bias for glucose for each model. The lowest bias was -0.06 for the high correlation level model. 
```{r, echo=FALSE}

true_mean_glucose <- 0 #because data was centered around 0

#variance
calculate_variance <- function(model) {
  return(diag(vcov(model)))
}

#bias for glucose 
calculate_bias_glucose <- function(model, true_mean_glucose, data) {
  predicted_mean_glucose <- predict(model, newdata = data.frame(
    cholesterol = 0,  # all centered
    bmi = 0,          
    blood_pressure = 0
  ))
  
  #bias = predicted mean - true mean
  bias <- predicted_mean_glucose - true_mean_glucose
  return(bias)
}

#store results
store_results <- function(model, scenario_name, true_mean_glucose, data) {
  variance <- calculate_variance(model)
  bias <- calculate_bias_glucose(model, true_mean_glucose, data)
  
  data.frame(
    Scenario = scenario_name,
    Variance_Intercept = variance[1],
    Variance_BloodPressure = variance[2],
    Variance_Cholesterol = variance[3],
    Variance_BMI = variance[4],
    Bias_Glucose = bias
  )
}

#store results for each scenario
results <- rbind(
  store_results(original_model, "Original Data", true_mean_glucose, original_data),
  store_results(large_model, "Larger Sample Size", true_mean_glucose, large_data),
  store_results(high_corr_model, "High Correlation Levels", true_mean_glucose, high_corr_data),
  store_results(low_corr_model, "Low Correlation Levels", true_mean_glucose, low_corr_data),
  store_results(high_df_model, "High df (Less Non-Normal)", true_mean_glucose, high_df_data),
  store_results(low_df_model, "Low df (More Non-Normal)", true_mean_glucose, low_df_data),
  store_results(bmi_model, "BMI Effect Model", true_mean_glucose, bmi_data)
)

rownames(results) <- NULL
#results table
kable(results, caption = "Comparison of Variance and Bias Across Different Scenarios")
```

The table below show the R squared value for each model and the significant predictors based on p-value.
```{r, echo=FALSE}
#extract R^2 and significant predictors
extract_results <- function(model, scenario_name) {
  r_squared <- summary(model)$r.squared
  
#coefficients and p-values
  coef_table <- summary(model)$coefficients
  significant_predictors <- rownames(coef_table)[coef_table[, "Pr(>|t|)"] < 0.05]
  
  significant_predictors <- significant_predictors[significant_predictors != "(Intercept)"]
  
  data.frame(
    Scenario = scenario_name,
    R_Squared = r_squared,
    Significant_Predictors = paste(significant_predictors, collapse = ", ")
  )
}

results_table <- rbind(
  extract_results(original_model, "Original Data"),
  extract_results(large_model, "Larger Sample Size"),
  extract_results(high_corr_model, "High Correlation Levels"),
  extract_results(low_corr_model, "Low Correlation Levels"),
  extract_results(high_df_model, "High df (Less Non-Normal)"),
  extract_results(low_df_model, "Low df (More Non-Normal)"),
  extract_results(bmi_model, "BMI Effect")
)

rownames(results) <- NULL

kable(results_table, caption = "Model Performance and Significant Predictors")
```

The graph below shows which models had the most variance for the parameter coefficients. 
```{r, echo=FALSE}
#variance (for coefficients)
variance_long <- results %>%
  pivot_longer(cols = starts_with("Variance"), 
               names_to = "Coefficient", 
               values_to = "Variance")

ggplot(variance_long, aes(x = Scenario, y = Variance, fill = Coefficient)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Variance of Coefficients", x = "Scenario", y = "Variance") +
  theme_minimal()
```
### Final models for each simulation:

Original data:
$$
Y = 0.267*Cholesterol + \epsilon
$$

Large sample size:
$$
Y = 0.298*Cholesterol + \epsilon
$$

High correlation:
$$
Y = 0.456*Cholesterol + \epsilon
$$

Low correlation: No significant predictors


High degrees of freedom (more skewed):
$$
Y = 0.302*Cholesterol + \epsilon
$$

Low degrees of freedom (less skewed):
$$
Y = 0.282*Cholesterol + \epsilon
$$

BMI correlation:
$$
Y = 0.267*Cholesterol + 1.6* BMI + \epsilon
$$


Finally, below compares the bias for each model's predicted value of glucose.
```{r, echo=FALSE}
#bias (for glucose)
ggplot(results, aes(x = Scenario, y = Bias_Glucose, fill = Scenario)) +
  geom_bar(stat = "identity") +
  labs(title = "Bias of Glucose Predictions", x = "Scenario", y = "Bias") +
  theme_minimal()
```

## Discussion of models for simulation 2

Overall, the MM robust regression method performed well across all of the above senarios. All of them converged and the generated model held. For the increase in smple size, the variance was reduced and the R squared value increased which is consistent with theory. Bias also increased. Increasing the correlation among predictors reduced variance and improved model fit (increased R Squared). When correlation was lowered, none of the predictors were significant, variance was increased, and the model did not git as well. When changing degrees of normality, the lower degree of freedom model increased bias and slightly improved the model fit, and increased degrees of freedom reduced bias but have little effect on the model fit. Lastly, the BMI effect model increased model fit, and the bias is less negative the the original data. 

# Discussion
# Key Takeaways
Key Findings:
Appropriateness of copula transformation to induce correlation
Simulation Study 1
When data violate model assumptions of iid and normality...
Increasing sample size does not reduce bias
Increases the probability of a Type I error
More skewness/a greater deviation from normality in the data leads to...
More bias in regression coefficient estimates
Increased risk of a Type I error
\newline
Simulation Study 2
\newlinw
The correlated variable was the most significant in every model
Highest R squared value was for the simulation with the highest correlation. MM- robust regression was succesfull for all simulations since it acounts for heavy-dated and correlated data. 
\newline
Take-home messages & practical recommendations:
\newline
Correct specification of a model is crucial to valid parameter estimates and statistical inference. When model assumptions are violated by the underlying correlation or distributional structures of the data, models which assume iid & Normal data produce invalid results.With knowledge of the underlying correlation and distributional structures, appropriate models can be chosen to achieve unbiased estimates and valid inference

# Appendix: Robust Regression Outputs
### Original Data 
```{r,echo=FALSE}
set.seed(2025)

original_data <- generate_t_copula_data(n = 100, nu = 5)
original_data <- original_data %>%
  mutate(across(everything(), ~ . - mean(.)))#centering the data. this prevents the intercept variance from over affecting the data 
original_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = original_data)
summary(original_model) #robust regression using MM
```
### Larger Sample Size

```{r,echo=FALSE}
set.seed(2025)

large_data <- generate_t_copula_data(n = 500, nu = 5)
large_data <- large_data %>%
  mutate(across(everything(), ~ . - mean(.)))
large_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = large_data)
summary(large_model)
```
### Varying Correlation Levels

```{r, echo=FALSE}
set.seed(2025)
high_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.8, 0.7))
high_corr_data <- high_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_corr_data)
summary(high_corr_model)

low_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.2, 0.1))
low_corr_data <- low_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_corr_data)
summary(low_corr_model)
```

### Varying Non-Normality Degree
```{r, echo=FALSE}
low_df_data <- generate_t_copula_data(n = 100, nu = 4)
low_df_data <-low_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_df_data)
summary(low_df_model)

high_df_data <- generate_t_copula_data(n = 100, nu = 6)
high_df_data <- high_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_df_data)
summary(high_df_model)

```
### BMI Correlation
```{r, echo=FALSE}
set.seed(2025)
bmi_data <- generate_t_copula_data(n = 100, nu = 5)

#Add a strong linear effect of bmi on glucose
bmi_data$glucose <- bmi_data$glucose + 2 * bmi_data$bmi  # Adjust the coefficient (0.5) as needed

#center
bmi_data <- bmi_data %>% mutate(across(everything(), ~ . - mean(.)))

#robust regression model
bmi_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = bmi_data)
summary(bmi_model)
```

# Appendix: Full Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

## References


Ahmadi, A., Baghfalaki, T., Ganjali, M., Kabir, A., & Pazouki, A. (2021). A transition copula model for analyzing multivariate longitudinal data with missing responses. Journal of applied statistics, 49(12), 3164–3177. https://doi.org/10.1080/02664763.2021.1931055  

Palomar, D. P., Zhou, R., & Wang, X. (2023, April 29). Mean Vector and Covariance Matrix Estimation under Heavy Tails. https://cran.r-project.org/web/packages/fitHeavyTail/vignettes/CovarianceEstimationHeavyTail.html

Huber, P. J. (1981). Robust Statistics. Wiley.

Yohai, V. J. (1987). High breakdown-point and high efficiency robust estimates for regression. The Annals of Statistics, 15(2), 642–656.

Rousseeuw, P. J., & Leroy, A. M. (1987). Robust Regression and Outlier Detection. Wiley.