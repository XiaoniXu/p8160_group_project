---
title: "Project 1: Multivariate Non-Normal Distributions and Correlated Data"
output: html_document
---

# 1. Plan and specify distributions and correlation

We simulate medical data with heavy-tailed a Multivariate t-Distribution using a t-Copula to generate correlated variables: Glucose, Cholesterol, BMI, and Blood Pressure).  


```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(MASS)  
library(Matrix)
library(copula)
library(mvtnorm)
library(ggplot2) 
library(GGally)
library(DT)
library(patchwork)
```


```{r set_parameters, message=FALSE}
set.seed(2025)

# Define parameters
nu <- 3  # Degrees of freedom (controls tail thickness)
p <- 4   # Number of variables
n <- 1000  # Number of samples

# Define mean vector
mu <- c(100,   # Mean Glucose Level (mg/dL)
        200,   # Mean Cholesterol Level (mg/dL)
        25,    # Mean BMI (Body Mass Index)
        120)   # Mean Blood Pressure (mmHg)
```

- Degrees of Freedom (nu = 5)

Lower degrees of freedom results in a heavier tail, which better simulates extreme cases in medical data (diabetes, obesity, etc.).  


- Number of Variables (p = 4)

These four variables (Glucose, Cholesterol, BMI, Blood Pressure) are commonly used metabolic and cardiovascular health indicators and are correlated to each other.

- Choice of Mean Values (mu)  
The mean values are set based on typical values of a healthy adult.


```{r corr_structure}
# Define block structures
# Block-diagonal: high correlation within groups, no correlation across groups

# Glucose & Cholesterol (Metabolism)
block1 <- matrix(c(1, 0.5, 
                   0.5, 1), nrow=2, ncol=2) 
# BMI & Blood Pressure (Cardiovascular)
block2 <- matrix(c(1, 0.4, 
                   0.4, 1), nrow=2, ncol=2)  

# Combine blocks into a block-diagonal matrix
Sigma <- bdiag(block1, block2)

# Convert to a standard matrix format
# Ensure the correlation matrix is positive definite
Sigma <- as.matrix(nearPD(Sigma)$mat) 
```

Copulas are the mechanism which allows us to isolate the dependency structure in a multivariate distribution. In particular, we can construct any multivariate distribution by separately specifying the marginal distributions and the copula. (Haugh, 2016) https://www.columbia.edu/~mh2078/QRM/Copulas.pdf

Idea behind copulas:

- It is a mathematical object that has input as 2 or more marginal distributions and has output as a joint  distribution

- If we have two probabilities that live on 0 -> 1, we can transform them into a higher space of 0 to infinity. Then when we add them together, they still live in the space of 0 to infinity. We take the inverse transformation of the sum, so the sum of the two probabilities will live on 0 -> 1 again.

- If we use a negative log transformation (which will put any probability from 0 to 1 on a scale from 0 to infinity), the inverse will be negative exponential transformation (which brings anything from 0 to infinity back to 0 to 1).

- If the two probabilities are independent, the joint probability is equal to the product of the individual probabilities (Frechet-Hoffding boundary copula). We're not so interested in it because we want to use copulas for distributions with correlation.

- Copula allows distributions that have dependency to have flexibility. e.g. Gumbel copula - dependency increases with extreme positive values, which captures the dependency better than any multivariate distribution.

- Gaussian copula transforms variables to standard normal distributions (mean = 0, variance = 1) by mapping them on a percentile-to-percentile basis. Those distributions are then combined to form a joint distribution 

- The t-copula is derived from the multivariate t-distribution, just as the Gaussian copula comes from the multivariate normal. t-copula better captures the phenomenon of dependent extreme values (tail dependence)

- Tail dependence is when the correlation between two variables increases as you get "further" in the tail (either or both) of the distribution. Gaussian copula has no tail dependence.

<<<<<<< HEAD
----------------------------to delete------------------------------  
=======
----------------------------to delete------------------------------
>>>>>>> 44fb48bc34e045e0266b243015df6502e256583d
Medical Justification for Grouping Variables:  

1. Glucose & Cholesterol (Metabolic Block)  
These two variables are often correlated because they are both regulated by insulin and metabolic processes.  
Patients with high glucose levels (e.g., in diabetes) often have elevated cholesterol due to metabolic dysfunction.  

2. BMI & Blood Pressure (Cardiovascular Block)  
Higher BMI is strongly associated with increased blood pressure, as obesity contributes to hypertension and cardiovascular disease.  
This correlation reflects real-world epidemiological findings in obesity and hypertension studies.  

```{r t_copula_generation}
# Extract lower triangle correlations
rho_vec <- Sigma[lower.tri(Sigma)]  

# Define the t-Copula
copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

# Generate Copula-Based Uniform Samples
U_t <- rCopula(1000, copula_t) 

# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

# Generate Independent Heavy-Tailed t-Distributions
glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]

# Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]


# Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
data_copula_t <- data.frame(
  Glucose = quantile(glucose, probs = U_t[,1]),
  Cholesterol = quantile(cholesterol, probs = U_t[,2]),
  BMI = quantile(BMI, probs = U_t[,3]),
  BloodPressure = quantile(blood_pressure, probs = U_t[,4])
)

# Generate correlation matrix
corr_matrix <- cor(data_copula_t) %>% as.data.frame()
knitr::kable(corr_matrix, caption = "Correlation Matrix of t_copula", digits = 3)
```

<<<<<<< HEAD
----------------------------to delete------------------------------  
=======
----------------------------to delete------------------------------
>>>>>>> 44fb48bc34e045e0266b243015df6502e256583d

A t-Copula is ideal for modeling correlated medical data with heavy tails, capturing realistic dependencies between variables.  

Medical variables (glucose, cholesterol, BMI, blood pressure) often show heavy-tailed distributions.  
The t-Copula ensures that extreme values tend to co-occur, reflecting real-world patterns (e.g., high glucose & high cholesterol in diabetes).  

A Gaussian Copula assumes weak tail dependence, failing to model joint extremes properly.  

```{r visualize_t_copula}
ggpairs(data_copula_t, 
        title = "t-Copula: Multivariate t-Distribution",
        lower = list(continuous = "points"),
        diag = list(continuous = "densityDiag"),
        upper = list(continuous = "cor")) +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14))
```



# 2. Implement your data generation method

```{r generate_data}
# Function for generate multivariate t-distribution data 
generate_t_copula_data <- function(n = n, nu = nu) {
  
  # Define block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 
                     0.5, 1), nrow=2, ncol=2)  # Metabolic Block (Glucose & Cholesterol)
  block2 <- matrix(c(1, 0.4, 
                     0.4, 1), nrow=2, ncol=2)  # Cardiovascular Block (BMI & Blood Pressure)

  # Combine blocks into a correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure it is positive definite

  # Extract lower triangle correlations for t-Copula
  rho_vec <- Sigma[lower.tri(Sigma)]  

  # Define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

  # Generate Copula-Based Uniform Samples
  U_t <- rCopula(n, copula_t) 

  # Define upper and lower bounds for each variable
  glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
  glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL
  
  cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
  cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL
  
  BMI_min <- 15    # minimum plausible BMI
  BMI_max <- 40    # maximum plausible BMI
  
  blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
  blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg
  
  # Generate Independent Heavy-Tailed t-Distributions
  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
  glucose <- glucose_raw[glucose_raw >= glucose_min & 
                           glucose_raw <= glucose_max]
  cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min 
                                 & cholesterol_raw <= cholesterol_max]
  BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
  blood_pressure <- blood_pressure_raw[blood_pressure_raw 
                                       >= blood_pressure_min & 
                                         blood_pressure_raw <= 
                                         blood_pressure_max]
  
  
  # Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
  data_copula_t <- data.frame(
    Glucose = quantile(glucose, probs = U_t[,1]),
    Cholesterol = quantile(cholesterol, probs = U_t[,2]),
    BMI = quantile(BMI, probs = U_t[,3]),
    BloodPressure = quantile(blood_pressure, probs = U_t[,4])
  )

  return(data_copula_t)
}

# Generate a dataset with 1000 samples and df = 5
generated_data <- generate_t_copula_data(n = n, nu = nu)

# Save data as CSV
write.csv(generated_data, "generated_t_copula_data.csv", row.names = FALSE)
```


```{r visualization_hist_fit}
plot_distribution <- function(data, variable_name, df=5) {
  x_min <- min(data[[variable_name]])
  x_max <- max(data[[variable_name]])
  mean_val <- mean(data[[variable_name]])
  sd_val <- sd(data[[variable_name]])
  
  ggplot(data, aes(x = .data[[variable_name]])) +  
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +  
    stat_function(fun = function(x) dt((x - mean_val) / sd_val, df = df) / sd_val, 
                  col = "red", linewidth = .8) +  
    labs(title = paste("Histogram & t-Dist Fit for", variable_name), 
         x = variable_name, y = "Density") +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0))
}

p1 <- plot_distribution(generated_data, "Glucose")
p2 <- plot_distribution(generated_data, "Cholesterol")
p3 <- plot_distribution(generated_data, "BMI")
p4 <- plot_distribution(generated_data, "BloodPressure")

(p1 + p2) / (p3 + p4)
```

This visualization compares the empirical distributions of the generated data (blue histograms) with the fitted t-distributions (red curves).  

The t-distribution fits well with the generated data with heavy-tailed distributions both at the center and on the tails.

Overall, the t-Copula effectively models correlations and heavy tails, making it a suitable choice for realistic medical data simulation.  


```{r}
# Compute the empirical correlation matrix
empirical_corr <- cor(data_copula_t)

# Display side-by-side comparison
comparison_matrix <- data.frame(
  Variable_1 = c("Glucose", "Glucose", "Glucose", 
                 "Cholesterol", "Cholesterol", "BMI"),
  Variable_2 = c("Cholesterol", "BMI", "Blood Pressure", 
                 "BMI", "Blood Pressure", "Blood Pressure"),
  Target_Correlation = c(0.5, 0, 0, 0, 0, 0.4),  # From defined Sigma
  Empirical_Correlation = c(
    empirical_corr["Glucose", "Cholesterol"],
    empirical_corr["Glucose", "BMI"],
    empirical_corr["Glucose", "BloodPressure"],
    empirical_corr["Cholesterol", "BMI"],
    empirical_corr["Cholesterol", "BloodPressure"],
    empirical_corr["BMI", "BloodPressure"]
  )
)

# Display the table
knitr::kable(comparison_matrix, 
             caption = "Comparison of Target vs. Empirical Correlations", 
             digits = 3)
```

# Part 3

## Simulation Study 1

*Note: we have done this for a multivariable dist. with 4 variables rather than 2... harder to visualize/keep track of all of the different covars (since there are 6 pairwise correlations/regressions)*

### 3.1 Fit a regression model to the simulated data while assuming that data are i.i.d and normally distributed

```{r simstudy1_1}
# Linear regression -- assumes iid and X ~ N() [i.e. errors/residuals ~ N()]

# Generate a dataset with 1000 samples and df = 5
nn_3_1 = 1000
df_3_1 = 5
generated_data_3_1 <- generate_t_copula_data(n = nn_3_1, nu = df_3_1)

# function for storing beta & 95% CI from each pairwise reg
pairwise_linreg_storebeta <- function(gen_data, samp_size){
  lm_out <- list()
  for (ii in colnames(gen_data)){
    for (jj in colnames(gen_data)){
      # same variable
      if (ii == jj){ # pass
      }
      # 6 unique pairwise regressions for four variables (3 + 2 + 1)
      else{
        linreg <- lm(gen_data[[ii]] ~ gen_data[[jj]]) # ii = y ~ jj = x
        beta <- linreg$coefficients[2] # regression coefficient
        lower <- confint(linreg)[2] # UB & LB of 95% CI for reg coeff
        upper <- confint(linreg)[4]
        # store output, named
        lm_out[[paste0(ii, "_", jj)]] <- c(vars = paste0(ii,"_", jj), samp_size = samp_size, beta=beta, lower=lower, upper=upper)  # stored as y_x
      }
    }
  }
  return(lm_out)
}
```

### 3.2 Conduct a simulation study that varies sample sizes, correlation levels, or degree of non-normality to assess bias in parameter estimates, type I error in hypothesis testing and coverage in conference intervals.

*Interpretation: 'parameter estimates' refers to correlation only (not beta or intercept for simple linear regression)*

* Note: as per instructions, I have fit a regression model. However, the easier way to calculate this would be with cor() in R. Is there a theoretical solution for the linear regression output? In that case, assessing CIs & bias would be simple. However, since I'm not 100% clear on the code under "# Generate Independent Heavy-Tailed t-Distributions", I'm not clear what those theoretical values would be... 
* Will be very easy to plot the single theoretical value when I have that cleared up [and will be obvious whether the 95% CI includes/covers it]

```{r simstudy1_2}
# 3.2.1 Vary sample size

# 10 different sample sizes -- explore different orders of magnitude of sample size, rounded
sampsize_3_2 <- ceiling(10^seq(1,4.5,length.out=10))

# generate samples & store correlations
# degree of skewness/variation from normality is fixed for these simulations (df)
df_3_2 = 5
lm_varysampsize_store <- list()

# assess bias in parameter estimates, CI coverage (true value included/not included in 95% CI) [implies future Type I/II errors]

# neater approach: extract beta & 95% CI [**note: under assumption of ~ iid, N(] (!)
for (ii in sampsize_3_2){
  generated_data_loop <- generate_t_copula_data(n = ii, nu = df_3_2)
  lm_varysampsize_store[[paste0("sampsize", ii)]] <- c(pairwise_linreg_storebeta(generated_data_loop, samp_size = ii))
}

# collapse & plot all
testing <- as.data.frame(t(as.data.frame((c(lm_varysampsize_store))))) # 120 obs-nice
rownames(testing) <- seq(1,nrow(testing))
colnames(testing)[3] <- "beta"
testing$beta <- as.numeric(testing$beta)
testing$upper <- as.numeric(testing$upper)
testing$lower <- as.numeric(testing$lower)

# plot -- examples...
testing_plot <- testing |> filter(vars == "Glucose_Cholesterol")
ggplot(data=testing_plot, aes(x = as.numeric(samp_size), y = beta)) +

  geom_point() +

  geom_errorbar(aes(ymin = as.numeric(lower), ymax = as.numeric(upper)), width = 0.2, position = position_dodge(width = 0.9)) + 
  
  facet_grid(rows = vars(vars)) + 

  scale_x_log10() 

# and for the other relationship of interest
testing_plot1 <- testing |> filter(vars == "BMI_BloodPressure")
ggplot(data=testing_plot1, aes(x = as.numeric(samp_size), y = beta)) +

  geom_point() +

  geom_errorbar(aes(ymin = as.numeric(lower), ymax = as.numeric(upper)), width = 0.2, position = position_dodge(width = 0.9)) + 
  
  facet_grid(rows = vars(vars)) + 

  scale_x_log10() 

# would prefer to plot them all in facets, if we are sticking with the 4 variables/6 pairwise correlations

# would add a HORIZONTAL LINE to each at the true theoretical value
```


```{r simstudy1_2_samples}
# Multiple iterations for each sample size -- i.e. what ~percent~ of the CIs included the true value, etc. (build this out into a table)

# function which 1. generates data for a given sample size, 2. generates the linear regression, then 3. characterizes a) the mean beta estimated across those iterations for that sample size and b) the percentage of confidence intervals which covered the true value

confidence_coverage <- function(num_iterations){
  list_iterations <- list()
  for (iterations in seq(1,num_iterations)){
        for (ii in sampsize_3_2){
          generated_data_loop <- generate_t_copula_data(n = ii, nu = df_3_2)
          lm_varysampsize_store[[paste0("sampsize", ii)]] <- c(pairwise_linreg_storebeta(generated_data_loop, samp_size = ii))
        }
        
        # collapse & plot all
        one_sampsize <- as.data.frame(t(as.data.frame((c(lm_varysampsize_store))))) # 120 obs-nice
        rownames(one_sampsize) <- seq(1,nrow(one_sampsize))
        colnames(one_sampsize)[3] <- "beta"
        one_sampsize$beta <- as.numeric(one_sampsize$beta)
        one_sampsize$upper <- as.numeric(one_sampsize$upper)
        one_sampsize$lower <- as.numeric(one_sampsize$lower)
        
        # save output into list
        list_iterations[[iterations]] <- one_sampsize
        return(list_iterations)
  }
  
}

output_3_2 <- confidence_coverage(10) # somehthing is wrong here, i should be getting a list of list of lists lol FIX IT
# then the issue will just be compressing them to get the summary stats, but that's not SO complicated
```


* Some additional thoughts:
  * Should I be doing more than one realization of, for ex, n = 1000? that would add another layer of uncertainty... *but* that would mean that we could characterize, for ex, the % of the time that the CI covers the true value [for each n] -- *I think that actually is the way to go*
  * need to get theoretical regression coefficients in order to compare these CIs


