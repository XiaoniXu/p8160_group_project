---
title: "Project 1: Multivariate Non-Normal Distributions and Correlated Data"
output: html_document
---

# Hello this is the report for the project 1

# 1. Plan and specify distributions and correlation

We simulate medical data with heavy-tailed a Multivariate t-Distribution using a t-Copula to generate correlated variables: Glucose, Cholesterol, BMI, and Blood Pressure).  


```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(MASS)  
library(Matrix)
library(copula)
library(mvtnorm)
library(ggplot2) 
library(GGally)
library(DT)
library(patchwork)
```


```{r set_parameters, message=FALSE}
set.seed(2025)

# Define parameters
nu <- 3  # Degrees of freedom (controls tail thickness)
p <- 4   # Number of variables
n <- 1000  # Number of samples

# Define mean vector
mu <- c(100,   # Mean Glucose Level (mg/dL)
        200,   # Mean Cholesterol Level (mg/dL)
        25,    # Mean BMI (Body Mass Index)
        120)   # Mean Blood Pressure (mmHg)
```

- Degrees of Freedom (nu = 5)

Lower degrees of freedom results in a heavier tail, which better simulates extreme cases in medical data (diabetes, obesity, etc.).  


- Number of Variables (p = 4)

These four variables (Glucose, Cholesterol, BMI, Blood Pressure) are commonly used metabolic and cardiovascular health indicators and are correlated to each other.

- Choice of Mean Values (mu)  
The mean values are set based on typical values of a healthy adult.


```{r corr_structure}
# Define block structures
# Block-diagonal: high correlation within groups, no correlation across groups

# Glucose & Cholesterol (Metabolism)
block1 <- matrix(c(1, 0.5, 
                   0.5, 1), nrow=2, ncol=2) 
# BMI & Blood Pressure (Cardiovascular)
block2 <- matrix(c(1, 0.4, 
                   0.4, 1), nrow=2, ncol=2)  


# Combine blocks into a block-diagonal matrix
Sigma <- bdiag(block1, block2)

# Convert to a standard matrix format
# Ensure the correlation matrix is positive definite
Sigma <- as.matrix(nearPD(Sigma)$mat) 
```

Copulas are the mechanism which allows us to isolate the dependency structure in a multivariate distribution. In particular, we can construct any multivariate distribution by separately specifying the marginal distributions and the copula. (Haugh, 2016) https://www.columbia.edu/~mh2078/QRM/Copulas.pdf

Idea behind copulas:

- It is a mathematical object that has input as 2 or more marginal distributions and has output as a joint  distribution

- If we have two probabilities that live on 0 -> 1, we can transform them into a higher space of 0 to infinity. Then when we add them together, they still live in the space of 0 to infinity. We take the inverse transformation of the sum, so the sum of the two probabilities will live on 0 -> 1 again.

- If we use a negative log transformation (which will put any probability from 0 to 1 on a scale from 0 to infinity), the inverse will be negative exponential transformation (which brings anything from 0 to infinity back to 0 to 1).

- If the two probabilities are independent, the joint probability is equal to the product of the individual probabilities (Frechet-Hoffding boundary copula). We're not so interested in it because we want to use copulas for distributions with correlation.

- Copula allows distributions that have dependency to have flexibility. e.g. Gumbel copula - dependency increases with extreme positive values, which captures the dependency better than any multivariate distribution.

- Gaussian copula transforms variables to standard normal distributions (mean = 0, variance = 1) by mapping them on a percentile-to-percentile basis. Those distributions are then combined to form a joint distribution 

- The t-copula is derived from the multivariate t-distribution, just as the Gaussian copula comes from the multivariate normal. t-copula better captures the phenomenon of dependent extreme values (tail dependence)

- Tail dependence is when the correlation between two variables increases as you get "further" in the tail (either or both) of the distribution. Gaussian copula has no tail dependence.


Medical Justification for Grouping Variables:  

1. Glucose & Cholesterol (Metabolic Block)  
These two variables are often correlated because they are both regulated by insulin and metabolic processes.  
Patients with high glucose levels (e.g., in diabetes) often have elevated cholesterol due to metabolic dysfunction.  

2. BMI & Blood Pressure (Cardiovascular Block)  
Higher BMI is strongly associated with increased blood pressure, as obesity contributes to hypertension and cardiovascular disease.  
This correlation reflects real-world epidemiological findings in obesity and hypertension studies.  

```{r t_copula_generation}
# Extract lower triangle correlations
rho_vec <- Sigma[lower.tri(Sigma)]  

# Define the t-Copula
copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

# Generate Copula-Based Uniform Samples
U_t <- rCopula(1000, copula_t) 

# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

# Generate Independent Heavy-Tailed t-Distributions
glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]

# Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]


# Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
data_copula_t <- data.frame(
  Glucose = quantile(glucose, probs = U_t[,1]),
  Cholesterol = quantile(cholesterol, probs = U_t[,2]),
  BMI = quantile(BMI, probs = U_t[,3]),
  BloodPressure = quantile(blood_pressure, probs = U_t[,4])
)

# Generate correlation matrix
corr_matrix <- cor(data_copula_t) %>% as.data.frame()
knitr::kable(corr_matrix, caption = "Correlation Matrix of t_copula", digits = 3)
```


A t-Copula is ideal for modeling correlated medical data with heavy tails, capturing realistic dependencies between variables.  

Medical variables (glucose, cholesterol, BMI, blood pressure) often show heavy-tailed distributions.  
The t-Copula ensures that extreme values tend to co-occur, reflecting real-world patterns (e.g., high glucose & high cholesterol in diabetes).  

A Gaussian Copula assumes weak tail dependence, failing to model joint extremes properly.  

```{r visualize_t_copula}
ggpairs(data_copula_t, 
        title = "t-Copula: Multivariate t-Distribution",
        lower = list(continuous = "points"),
        diag = list(continuous = "densityDiag"),
        upper = list(continuous = "cor")) +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14))
```



# 2. Implement your data generation method

```{r generate_data}
# Function for generate multivariate t-distribution data 
generate_t_copula_data <- function(n = n, nu = nu) {
  
  # Define block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 
                     0.5, 1), nrow=2, ncol=2)  # Metabolic Block (Glucose & Cholesterol)
  block2 <- matrix(c(1, 0.4, 
                     0.4, 1), nrow=2, ncol=2)  # Cardiovascular Block (BMI & Blood Pressure)

  # Combine blocks into a correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure it is positive definite

  # Extract lower triangle correlations for t-Copula
  rho_vec <- Sigma[lower.tri(Sigma)]  

  # Define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

  # Generate Copula-Based Uniform Samples
  U_t <- rCopula(n, copula_t) 

  # Define upper and lower bounds for each variable
  glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
  glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL
  
  cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
  cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL
  
  BMI_min <- 15    # minimum plausible BMI
  BMI_max <- 40    # maximum plausible BMI
  
  blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
  blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg
  
  # Generate Independent Heavy-Tailed t-Distributions
  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
  glucose <- glucose_raw[glucose_raw >= glucose_min & 
                           glucose_raw <= glucose_max]
  cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min 
                                 & cholesterol_raw <= cholesterol_max]
  BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
  blood_pressure <- blood_pressure_raw[blood_pressure_raw 
                                       >= blood_pressure_min & 
                                         blood_pressure_raw <= 
                                         blood_pressure_max]
  
  
  # Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
  data_copula_t <- data.frame(
    Glucose = quantile(glucose, probs = U_t[,1]),
    Cholesterol = quantile(cholesterol, probs = U_t[,2]),
    BMI = quantile(BMI, probs = U_t[,3]),
    BloodPressure = quantile(blood_pressure, probs = U_t[,4])
  )

  return(data_copula_t)
}

# Generate a dataset with 1000 samples and df = 5
generated_data <- generate_t_copula_data(n = n, nu = nu)

# Save data as CSV
write.csv(generated_data, "generated_t_copula_data.csv", row.names = FALSE)
```


```{r visualization_hist_fit}
plot_distribution <- function(data, variable_name, df=5) {
  x_min <- min(data[[variable_name]])
  x_max <- max(data[[variable_name]])
  mean_val <- mean(data[[variable_name]])
  sd_val <- sd(data[[variable_name]])
  
  ggplot(data, aes(x = .data[[variable_name]])) +  
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +  
    stat_function(fun = function(x) dt((x - mean_val) / sd_val, df = df) / sd_val, 
                  col = "red", linewidth = .8) +  
    labs(title = paste("Histogram & t-Dist Fit for", variable_name), 
         x = variable_name, y = "Density") +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0))
}

p1 <- plot_distribution(generated_data, "Glucose")
p2 <- plot_distribution(generated_data, "Cholesterol")
p3 <- plot_distribution(generated_data, "BMI")
p4 <- plot_distribution(generated_data, "BloodPressure")

(p1 + p2) / (p3 + p4)
```

This visualization compares the empirical distributions of the generated data (blue histograms) with the fitted t-distributions (red curves).  

The t-distribution fits well with the generated data with heavy-tailed distributions both at the center and on the tails.

Overall, the t-Copula effectively models correlations and heavy tails, making it a suitable choice for realistic medical data simulation.  


```{r}
# Compute the empirical correlation matrix
empirical_corr <- cor(data_copula_t)

# Display side-by-side comparison
comparison_matrix <- data.frame(
  Variable_1 = c("Glucose", "Glucose", "Glucose", 
                 "Cholesterol", "Cholesterol", "BMI"),
  Variable_2 = c("Cholesterol", "BMI", "Blood Pressure", 
                 "BMI", "Blood Pressure", "Blood Pressure"),
  Target_Correlation = c(0.5, 0.2, 0.1, 0.1, 0.2, 0.4),  # From defined Sigma
  Empirical_Correlation = c(
    empirical_corr["Glucose", "Cholesterol"],
    empirical_corr["Glucose", "BMI"],
    empirical_corr["Glucose", "BloodPressure"],
    empirical_corr["Cholesterol", "BMI"],
    empirical_corr["Cholesterol", "BloodPressure"],
    empirical_corr["BMI", "BloodPressure"]
  )
)

# Display the table
knitr::kable(comparison_matrix, 
             caption = "Comparison of Target vs. Empirical Correlations", 
             digits = 3)

```


# Part 3

## Simulation Study 1

### Fit a regression model to the simulated data while assuming that data are i.i.d and normally distributed

### Conduct a simulation study that varies sample sizes, correlation levels, or degree of non-normality to assess bias in parameter estimates, type I error in hypothesis testing and coverage in conference intervals.

### Demonstrate how inference changes as the severity of assumption deviation increases (more skew, heavier tail, stronger correlation)

* *Interpretation: "vary sample size, correlation level, or degree of non-normality". In the code below, we choose from these three options: to vary sample size.*
* *Interpretation: "assess bias in parameter estimates, type I error in hypothesis testing, and coverage in CIs". In the code below, we assess these by a) calculating the average coefficient estimates under each setting, and b) calculating the percentage of 95% CIs [calculated under the incorrect assumption of normality] for each setting which include the true, theoretical value for a given regression coefficient.*
* *Note: Type I error in hypothesis testing (reject the null H0, when the null is in fact true) depends on the specific H0 being posited. In this case, we record below the percentage of cases where the null relationship [specified in our true correlation matrix, above] between Glucose (Y) & two of the X variables (BMI & Blood Pressure, X3 & X3) is _incorrectly_ characterized as a null relationship--i.e. when the 95% CIs for the regression coefficients of those two variables do not include 0. This is equivalent because, within the hypothesis testing framework, a 95% CI is equivalent to a p-value--so a "Type I error" is equivalent to a 95% CI _not_ including zero when it should, in fact, include zero.* ----> OH but: it is actually distinct, because we have evaluated it empirically... the number of times that the CI does NOT include the true value within the 1000 simulations is related to the Type I error! (also: extends cleanly to the non-zero value)

## 1. Vary (generated) sample size 

```{r simstudy1_1}
# Fit (multivariable) linear regression -- assumes iid and errors/residuals ~ N(), which can be violated with skewed/heavy-tailed distributions of explanatory or outcome variables (which is the case for all variables we have generated)

# clean environment
rm(list=setdiff(ls(), c("generate_t_copula_data","rho_vec", "p", "mu"))) # all I need for simulation study 1

# Get theoretical regression coefficients:

# ```{r generate_data} -- due to the work in this chunk, the standard deviation was rescaled (i.e. not dependent purely on t distribution df) -- numbers pulled out below:
sds_tdists <- c("glucose" = 15, "cholesterol" = 25, "BMI" = 3, "blood_pressure" = 10)
# regression coefficient = r*(sd_y/sd_x)
# for the 3 regression coefficients: Glucose-Cholesterol, Glucose-BMI, Glucose-BP: 
# Glucose-Cholesterol: 0.5 * 15/25
# Glucose-BMI, Glucose-BP: 0 (r=0)
target_reg_coeffs <- c("glucose_chol" = 0.5 * 15/25, "glucose_bmi" = 0, "glucose_bp" = 0)

# Now: choose one of the variables as the output, the other three as covariates. Choice is arbitrary, since this
# simulation study is a proof of concept/demonstration.

# Choose Glucose as Y; Cholesterol, BMI, Blood Pressure as X1, X2, X3

# show that the linreg can run:
gendata_ss1_test <- generate_t_copula_data(n = 100, nu = 3) # 3 df--lower df means higher skewness, but does not determine sd, due to the scaling performed above
lm(gendata_ss1_test$Glucose ~ gendata_ss1_test$Cholesterol + gendata_ss1_test$BMI + gendata_ss1_test$BloodPressure)
# It does a great job with the true beta1!...not so great with the covariates that should have been null
rm(gendata_ss1_test)
```


```{r}
# Vary sample size
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_sampsize <- ceiling(10^seq(1,4.5,length.out=10))

# generate samples & store correlations
tdist_df = 3 # arbitrary choice -- degree of skewness/variation from normality (t dist df) fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyss_storebetas <- function(fxn_gen, samp_sizes, tdist_degfree){ # give a fxn to generate data, list of sample sizes to vary through, & degrees of freedom("df") of t dists (fixed in this simulation)
 
   # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (nn in samp_sizes){
    
      gendata <- fxn_gen(n = nn, nu = tdist_df)
      # run regression
      lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
      
      # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
      # CIs
      lm_outputs <- as.data.frame(confint(lm_nn))
      colnames(lm_outputs) <- c("lower_95", "upper_95")
      lm_outputs <- lm_outputs[2:4,]
      rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
      # point values
      lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
      # marker for loop [generated sample size] & true values & pairname
      lm_outputs$gensampsize <- nn
      lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
      lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
      
      # store in list
      lm_out[[paste0("sampsize", nn)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
  }
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varysampsize <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyss_storebetas(fxn_gen = generate_t_copula_data, 
                                              samp_sizes = vary_sampsize, tdist_degfree = tdist_df)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim <- run_simstudy1_varysampsize(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim, "simstudy1_1000sim.csv", row.names = FALSE)
```

```{r}
# read in csv
library(here) # Rproject
simstudy1_1000sim <- read.csv(here("./simstudy_1/simstudy1_1000sim.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim$trueval_inCI <- (simstudy1_1000sim$true_val <= simstudy1_1000sim$upper_95) & (simstudy1_1000sim$true_val >= simstudy1_1000sim$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications


simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r}
# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)

ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y") + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("Empirical 95% CIs for coefficient estimate (1000 runs)")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y") + 
  scale_x_log10() +
  ggtitle("Percentage of 95% CIs [under normality] which included the true value")

# note: these results SEEM counterintuitive, but it makes sense!! with the sample size increasing, we *think* we are more precise--but as the CI shrinks and shrinks, it is less likely to include the true value (since we are using an inappropriate model) EVEN AS the model gets more and more confident in its estimate! FASCINATING
```
### Interpretation:
*still need to write up for the report, but my key points are in the slides right now*

## 2. Vary skewness

```{r}
# Vary skewness
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_skewness <- 3^seq(1,7)

# generate samples & store correlations
sampsize_gen = 500 # arbitrary choice -- generated sample size fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyskew_storebetas <- function(fxn_gen, samp_size, tdist_degfree){ # give a fxn to generate data, list of degrees of freedom to iterate through, & sample size for generated sample (fixed in this simulation)
 
   # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (skew in tdist_degfree){
    
      gendata <- fxn_gen(n = samp_size, nu = skew)
      # run regression
      lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
      
      # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
      # CIs
      lm_outputs <- as.data.frame(confint(lm_nn))
      colnames(lm_outputs) <- c("lower_95", "upper_95")
      lm_outputs <- lm_outputs[2:4,]
      rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
      # point values
      lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
      # marker for loop [skewness] & true values & pairname
      lm_outputs$tdist_df <- skew
      lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
      lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
      
      # store in list
      lm_out[[paste0("tdistdf", skew)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
  }
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varyskewness <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyskew_storebetas(fxn_gen = generate_t_copula_data, 
                                                samp_size = sampsize_gen, tdist_degfree = vary_skewness)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim_1 <- run_simstudy1_varyskewness(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim_1, "simstudy1_1000sim_1.csv", row.names = FALSE)
```

```{r}
# read in csv
simstudy1_1000sim <- read.csv(here("./simstudy_1/simstudy1_1000sim_1.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim_1$trueval_inCI <- (simstudy1_1000sim_1$true_val <= simstudy1_1000sim_1$upper_95) &
  (simstudy1_1000sim_1$true_val >= simstudy1_1000sim_1$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications


simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r}
# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)

ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y") + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_1_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("Empirical 95% CIs for coefficient estimate (1000 runs)")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y") + 
  scale_x_log10() +
  ggtitle("Percentage of 95% CIs [under normality] which included the true value")

```

