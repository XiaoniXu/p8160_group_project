---
title: "Project 1: Multivariate Non-Normal Distributions and Correlated Data"
output: html_document
---

# Hello this is the report for the project 1

# 1. Plan and specify distributions and correlation

We simulate medical data with heavy-tailed a Multivariate t-Distribution using a t-Copula to generate correlated variables: Glucose, Cholesterol, BMI, and Blood Pressure).  


```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(MASS)  
library(Matrix)
library(copula)
library(mvtnorm)
library(ggplot2) 
library(GGally)
library(DT)
library(patchwork)
library(kableExtra)
```


```{r set_parameters, message=FALSE}
set.seed(2025)

# Define parameters
nu <- 3  # Degrees of freedom (controls tail thickness)
p <- 4   # Number of variables
n <- 1000  # Number of samples

# Define mean vector
mu <- c(100,   # Mean Glucose Level (mg/dL)
        200,   # Mean Cholesterol Level (mg/dL)
        25,    # Mean BMI (Body Mass Index)
        120)   # Mean Blood Pressure (mmHg)
```

- Degrees of Freedom (nu = 5)

Lower degrees of freedom results in a heavier tail, which better simulates extreme cases in medical data (diabetes, obesity, etc.).  


- Number of Variables (p = 4)

These four variables (Glucose, Cholesterol, BMI, Blood Pressure) are commonly used metabolic and cardiovascular health indicators and are correlated to each other.

- Choice of Mean Values (mu)  
The mean values are set based on typical values of a healthy adult.


```{r corr_structure}
# Define block structures
# Block-diagonal: high correlation within groups, no correlation across groups

# Glucose & Cholesterol (Metabolism)
block1 <- matrix(c(1, 0.5, 
                   0.5, 1), nrow=2, ncol=2) 
# BMI & Blood Pressure (Cardiovascular)
block2 <- matrix(c(1, 0.4, 
                   0.4, 1), nrow=2, ncol=2)  


# Combine blocks into a block-diagonal matrix
Sigma <- bdiag(block1, block2)

# Convert to a standard matrix format
# Ensure the correlation matrix is positive definite
Sigma <- as.matrix(nearPD(Sigma)$mat) 
colnames(Sigma) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
rownames(Sigma) <- c("Glucose", "Cholesterol", "BMI", "BloodPressure")
df_sigma <- as.data.frame(Sigma)
knitr::kable(df_sigma, caption = "Correlation Matrix of Block-diagonal", digits = 3)
```

Copulas are the mechanism which allows us to isolate the dependency structure in a multivariate distribution. In particular, we can construct any multivariate distribution by separately specifying the marginal distributions and the copula. (Haugh, 2016) https://www.columbia.edu/~mh2078/QRM/Copulas.pdf

Idea behind copulas:

- It is a mathematical object that has input as 2 or more marginal distributions and has output as a joint  distribution

- If we have two probabilities that live on 0 -> 1, we can transform them into a higher space of 0 to infinity. Then when we add them together, they still live in the space of 0 to infinity. We take the inverse transformation of the sum, so the sum of the two probabilities will live on 0 -> 1 again.

- If we use a negative log transformation (which will put any probability from 0 to 1 on a scale from 0 to infinity), the inverse will be negative exponential transformation (which brings anything from 0 to infinity back to 0 to 1).

- If the two probabilities are independent, the joint probability is equal to the product of the individual probabilities (Frechet-Hoffding boundary copula). We're not so interested in it because we want to use copulas for distributions with correlation.

- Copula allows distributions that have dependency to have flexibility. e.g. Gumbel copula - dependency increases with extreme positive values, which captures the dependency better than any multivariate distribution.

- Gaussian copula transforms variables to standard normal distributions (mean = 0, variance = 1) by mapping them on a percentile-to-percentile basis. Those distributions are then combined to form a joint distribution 

- The t-copula is derived from the multivariate t-distribution, just as the Gaussian copula comes from the multivariate normal. t-copula better captures the phenomenon of dependent extreme values (tail dependence)

- Tail dependence is when the correlation between two variables increases as you get "further" in the tail (either or both) of the distribution. Gaussian copula has no tail dependence.


Medical Justification for Grouping Variables:  

1. Glucose & Cholesterol (Metabolic Block)  
These two variables are often correlated because they are both regulated by insulin and metabolic processes.  
Patients with high glucose levels (e.g., in diabetes) often have elevated cholesterol due to metabolic dysfunction.  

2. BMI & Blood Pressure (Cardiovascular Block)  
Higher BMI is strongly associated with increased blood pressure, as obesity contributes to hypertension and cardiovascular disease.  
This correlation reflects real-world epidemiological findings in obesity and hypertension studies.  

```{r t_copula_generation}
rho_vec <- Sigma[lower.tri(Sigma)]  

# Define the t-Copula
copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

# Generate Copula-Based Uniform Samples
U_t <- rCopula(1000, copula_t) 

# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

# Generate Independent Heavy-Tailed t-Distributions
glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]

# Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]


# Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
data_copula_t <- data.frame(
  Glucose = quantile(glucose, probs = U_t[,1]),
  Cholesterol = quantile(cholesterol, probs = U_t[,2]),
  BMI = quantile(BMI, probs = U_t[,3]),
  BloodPressure = quantile(blood_pressure, probs = U_t[,4])
)

# Generate correlation matrix
corr_matrix <- cor(data_copula_t) %>% as.data.frame()
knitr::kable(corr_matrix, caption = "Correlation Matrix of t_copula", digits = 3)
```


A t-Copula is ideal for modeling correlated medical data with heavy tails, capturing realistic dependencies between variables.  

Medical variables (glucose, cholesterol, BMI, blood pressure) often show heavy-tailed distributions.  
The t-Copula ensures that extreme values tend to co-occur, reflecting real-world patterns (e.g., high glucose & high cholesterol in diabetes).  

A Gaussian Copula assumes weak tail dependence, failing to model joint extremes properly.  

```{r visualize_t_copula}
ggpairs(data_copula_t, 
        title = "t-Copula: Multivariate t-Distribution",
        lower = list(continuous = "points"),
        diag = list(continuous = "densityDiag"),
        upper = list(continuous = "cor")) +
  theme(plot.title = element_text(hjust = 0.5, face="bold", size=14))
```



# 2. Implement your data generation method

```{r generate_data}
# Function for generate multivariate t-distribution data 
generate_t_copula_data <- function(n = n, nu = nu) {
  
  # Define block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 
                     0.5, 1), nrow=2, ncol=2)  # Metabolic Block (Glucose & Cholesterol)
  block2 <- matrix(c(1, 0.4, 
                     0.4, 1), nrow=2, ncol=2)  # Cardiovascular Block (BMI & Blood Pressure)

  # Combine blocks into a correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure it is positive definite

  # Extract lower triangle correlations for t-Copula
  rho_vec <- Sigma[lower.tri(Sigma)]  

  # Define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=p, df=nu, dispstr="un")

  # Generate Copula-Based Uniform Samples
  U_t <- rCopula(n, copula_t) 

  # Define upper and lower bounds for each variable
  glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
  glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL
  
  cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
  cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL
  
  BMI_min <- 15    # minimum plausible BMI
  BMI_max <- 40    # maximum plausible BMI
  
  blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
  blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg
  
  # Generate Independent Heavy-Tailed t-Distributions
  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  BMI_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
  glucose <- glucose_raw[glucose_raw >= glucose_min & 
                           glucose_raw <= glucose_max]
  cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min 
                                 & cholesterol_raw <= cholesterol_max]
  BMI <- BMI_raw[BMI_raw >= BMI_min & BMI_raw <= BMI_max]
  blood_pressure <- blood_pressure_raw[blood_pressure_raw 
                                       >= blood_pressure_min & 
                                         blood_pressure_raw <= 
                                         blood_pressure_max]
  
  
  # Map Copula Uniform Samples Back to Heavy-Tailed t-Distributions
  data_copula_t <- data.frame(
    Glucose = quantile(glucose, probs = U_t[,1]),
    Cholesterol = quantile(cholesterol, probs = U_t[,2]),
    BMI = quantile(BMI, probs = U_t[,3]),
    BloodPressure = quantile(blood_pressure, probs = U_t[,4])
  )

  return(data_copula_t)
}

# Generate a dataset with 1000 samples and df = 5
generated_data <- generate_t_copula_data(n = n, nu = nu)

# Save data as CSV
write.csv(generated_data, "generated_t_copula_data.csv", row.names = FALSE)
```


```{r visualization_hist_fit}
plot_distribution <- function(data, variable_name, df=5) {
  x_min <- min(data[[variable_name]])
  x_max <- max(data[[variable_name]])
  mean_val <- mean(data[[variable_name]])
  sd_val <- sd(data[[variable_name]])
  
  ggplot(data, aes(x = .data[[variable_name]])) +  
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +  
    stat_function(fun = function(x) dt((x - mean_val) / sd_val, df = df) / sd_val, 
                  col = "red", linewidth = .8) +  
    labs(title = paste("Histogram & t-Dist Fit for", variable_name), 
         x = variable_name, y = "Density") +
    theme_minimal() + 
    theme(plot.title = element_text(hjust = 0))
}

p1 <- plot_distribution(generated_data, "Glucose")
p2 <- plot_distribution(generated_data, "Cholesterol")
p3 <- plot_distribution(generated_data, "BMI")
p4 <- plot_distribution(generated_data, "BloodPressure")

(p1 + p2) / (p3 + p4)
```

This visualization compares the empirical distributions of the generated data (blue histograms) with the fitted t-distributions (red curves).  

The t-distribution fits well with the generated data with heavy-tailed distributions both at the center and on the tails.

Overall, the t-Copula effectively models correlations and heavy tails, making it a suitable choice for realistic medical data simulation.  


```{r}
# Compute the empirical correlation matrix
empirical_corr <- cor(data_copula_t)

# Display side-by-side comparison
comparison_matrix <- data.frame(
  Variable_1 = c("Glucose", "Glucose", "Glucose", 
                 "Cholesterol", "Cholesterol", "BMI"),
  Variable_2 = c("Cholesterol", "BMI", "Blood Pressure", 
                 "BMI", "Blood Pressure", "Blood Pressure"),
  Target_Correlation = c(0.5, 0.2, 0.1, 0.1, 0.2, 0.4),  # From defined Sigma
  Empirical_Correlation = c(
    empirical_corr["Glucose", "Cholesterol"],
    empirical_corr["Glucose", "BMI"],
    empirical_corr["Glucose", "BloodPressure"],
    empirical_corr["Cholesterol", "BMI"],
    empirical_corr["Cholesterol", "BloodPressure"],
    empirical_corr["BMI", "BloodPressure"]
  )
)

# Display the table
knitr::kable(comparison_matrix, 
             caption = "Comparison of Target vs. Empirical Correlations", 
             digits = 3)

```


# 3. Simulation Study 1

*"Fit a regression model to the simulated data while assuming that data are i.i.d and normally distributed. Conduct a simulation study that varies sample sizes, correlation levels, or degree of non-normality to assess bias in parameter estimates, type I error in hypothesis testing and coverage in conference intervals. Demonstrate how inference changes as the severity of assumption deviation increases (more skew, heavier tail, stronger correlation."*

* Interpretation of instructions: *"Vary sample size, correlation level, or degree of non-normality."* In the code below, we choose from two of these options: to vary 1. sample size and 2. skewness.
* Interpretation of instructions: *"Assess bias in parameter estimates, type I error in hypothesis testing, and coverage in CIs."* In the code below, we do the following for each simulation setting:
  1) Assess bias in parameter estimates by characterizing the mean estimated regression coefficients across n=1000 simulations, and by characterizing the empirical 95% CI of those estimates.
  2) Calculate the percentage of regression-fit 95% CIs [calculated under the incorrect assumption of normality] from the n=1000 simulationswhich included the true, theoretical value for a given regression coefficient. This gives us the 1 - [the empirical risk of a 'Type I error'], where Type I error can be defined as the probability that the regression-fit  95% CI will exclude the true value of the regression coefficient. Specifically: 1 - P(Type I error) = P(true value ~in~ 95% CI), where P(true value ~in~ 95% CI) is the proportion of the n=1000 simulations for which the true value *was* in the regression-fit 95% CI.
  
## Simulation studies and procedures
We choose as our regression model Multiple Linear Regression. In particular, we perform the following regression:
  
  \[ Y = \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 + \beta_3 * X_3 \]

where Y is Glucose, our outcome; X1 is Cholesterol, X2 is BMI, and X3 is Blood Pressure (BP).


Multiple linear regression (MLR) assumes a linear relationship between Y and all X, normally distributed residuals, and no multicollinearity in the set of covariates. The last two of these assumptions are both violated by correlated data we have generated. With skewed, heavy-tailed t-distributions for the independent and dependent variables, the model residuals are likely to not be non-normally distributed; and, with the correlation structure we induced, BMI and BP (two of our covariates) are highly correlated. We expect to see biased estimates and inference in using this regression structure to model our data, given that the data violate model assumptions.

We examined the effect of sample size (of the generated data samples) and skewness (of the underlying outcome and covariate distributions) on bias and inference (using MLR). 

a) First, we varied generated sample size to examine the effect of sample size on bias & inference. 
b) Second, we varied the degrees of freedom to the t-distribution to examine the effect of skewness (i.e. deviation from normality) on bias and inference.

### a. The effect of (generated) sample size on bias and inference, using a model which assumes *iid* and normally distributed data

- Simulation procedure: 
  - Vary through 10 sample sizes, across differing orders of magnitude
    - For each sample size, extract...
      - Estimated regression coefficient for each covariate (Cholesterol, BMI, BP) 
      - Whether or not the "normal" 95% CI included the true theoretical value for the regression coefficient for each covariate (Cholesterol, BMI, BP)
  - For each sample size setting: Simulate (repeat the above) n = 1000 times
    - For each covariate, we will summarize...
      - The mean estimated regression coefficient (across runs) & empirical 95% CI – *This gives us insight into the bias and variability of estimates provided by MLRs fit to the simulated samples of each sample size.*
      - The percentage of "normal" 95% CIs (% of runs) which included the true value – *This gives us insight into the probability of a Type I error, where the confidence interval generated by the the multiple linear regression does not include the true theoretical value. We assess how this changes with the sample size of the simulated samples.*
  
```{r simstudy1_1, include=FALSE}
# Fit (multivariable) linear regression -- assumes iid and errors/residuals ~ N(), which can be violated with skewed/heavy-tailed distributions of explanatory or outcome variables (which is the case for all variables we have generated)

# clean environment
rm(list=setdiff(ls(), c("generate_t_copula_data","rho_vec", "p", "mu"))) # all I need for simulation study 1

# Get theoretical regression coefficients:

# ```{r generate_data} -- due to the work in this chunk, the standard deviation was rescaled (i.e. not dependent purely on t distribution df) -- numbers pulled out below:
sds_tdists <- c("glucose" = 15, "cholesterol" = 25, "BMI" = 3, "blood_pressure" = 10)
# regression coefficient = r*(sd_y/sd_x)
# for the 3 regression coefficients: Glucose-Cholesterol, Glucose-BMI, Glucose-BP: 
# Glucose-Cholesterol: 0.5 * 15/25
# Glucose-BMI, Glucose-BP: 0 (r=0)
target_reg_coeffs <- c("glucose_chol" = 0.5 * 15/25, "glucose_bmi" = 0, "glucose_bp" = 0)

# Now: choose one of the variables as the output, the other three as covariates. Choice is arbitrary, since this
# simulation study is a proof of concept/demonstration.

# Choose Glucose as Y; Cholesterol, BMI, Blood Pressure as X1, X2, X3

# show that the linreg can run:
gendata_ss1_test <- generate_t_copula_data(n = 100, nu = 3) # 3 df--lower df means higher skewness, but does not determine sd, due to the scaling performed above
lm(gendata_ss1_test$Glucose ~ gendata_ss1_test$Cholesterol + gendata_ss1_test$BMI + gendata_ss1_test$BloodPressure)
# It does a great job with the true beta1!...not so great with the covariates that should have been null
rm(gendata_ss1_test)
```


```{r}
# Vary sample size
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_sampsize <- ceiling(10^seq(1,4.5,length.out=10))

# generate samples & store correlations
tdist_df = 3 # arbitrary choice -- degree of skewness/variation from normality (t dist df) fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyss_storebetas <- function(fxn_gen, samp_sizes, tdist_degfree){ # give a fxn to generate data, list of sample sizes to vary through, & degrees of freedom("df") of t dists (fixed in this simulation)
  
  # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (nn in samp_sizes){
    
    gendata <- fxn_gen(n = nn, nu = tdist_df)
    # run regression
    lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
    
    # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
    # CIs
    lm_outputs <- as.data.frame(confint(lm_nn))
    colnames(lm_outputs) <- c("lower_95", "upper_95")
    lm_outputs <- lm_outputs[2:4,]
    rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    # point values
    lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
    # marker for loop [generated sample size] & true values & pairname
    lm_outputs$gensampsize <- nn
    lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
    lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    
    # store in list
    lm_out[[paste0("sampsize", nn)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
}
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varysampsize <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyss_storebetas(fxn_gen = generate_t_copula_data, 
                                              samp_sizes = vary_sampsize, tdist_degfree = tdist_df)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r, include=FALSE}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim <- run_simstudy1_varysampsize(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim, "simstudy1_1000sim.csv", row.names = FALSE)
```

```{r, include=FALSE}
# read in csv
library(here) # Rproject
simstudy1_1000sim <- read.csv(here("./simstudy_1/simstudy1_1000sim.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim$trueval_inCI <- (simstudy1_1000sim$true_val <= simstudy1_1000sim$upper_95) & (simstudy1_1000sim$true_val >= simstudy1_1000sim$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications

simstudy1_1000sim_typeone <- simstudy1_1000sim |> 
  group_by(gensampsize, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r, echo =FALSE}
# Plotting

# facet labels
# facet

coeff_names <- c(
  'Gluc_BMI'="BMI",
  'Gluc_BP'="BP",
  'Gluc_Chol'="Cholesterol"
  )

# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)
ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("MLR coefficient estimates vs. Sample size") + 
  labs(x = "Generated sample size",
       y = "Mean reg. coeff. estimate across runs (empirical 95% CI)",
       subtitle = "n = 1000 runs. Green line indicates true value.",
       caption = "With increasing sample size of generated data, variation in MLR coefficient estimates
       decreased, and we observed the bias more clearly (non-consistent estimator.)")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_typeone, aes(x = gensampsize, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  ggtitle("1 - P(Type I Error) vs. Sample size, in the presence of bias") + 
  labs(x = "Generated sample size",
       y = "Proportion of runs where MLR 95% CI included true value",
       subtitle = "n = 1000 runs",
       caption = "Increasing sample size of generated data led to more incorrect inference; the model became 
       more confident in its estimates, while the mismatch between our model assumptions and data 
       generating mechanism (bias) remained constant.")

# note: these results SEEM counterintuitive, but it makes sense!! with the sample size increasing, we *think* we are more precise--but as the CI shrinks and shrinks, it is less likely to include the true value (since we are using an inappropriate model) EVEN AS the model gets more and more confident in its estimate! FASCINATING
```


_Interpretation:_

As sample sizes increased, the variation in the regression coefficient estimates estimated by the MLR decreased. This makes sense; with larger sample sizes, there would be less variability in the shape of the distributions generated with our data-generating mechanism (LLN), and so the relationship between those data would be more and more similar, leading to more consistent regression coefficient estimates between simulations. While the coefficients for BMI and BP were centered around their true values (indicating a lack of bias in these estimates), the bias for the estimate of the cholesterol coefficient grew more pronounced as sample size increased. Potential intuition for this could be that BMI and BP were, as specified in our true correlation matrix, completely uncorrelated with the outcome (Glucose). In contrast, the coefficient estimate for Cholesterol was meant to estimate a true value of 0.3, but was likely converging to a biased estimate as sample size increased due to non-normal residuals in the multiple linear regression (MLR).

As sample sizes increased, the probability of the MLR 95% CIs *including* the true regression coefficient values decreased, indicating that the probability of those CIs *excluding* the true regression coefficient values (i.e. the probability of a Type I error) increased. This trend was clear across all three covariates. The most likely explanation is that, as sample size increased, the MLR model became more confident in its estimates and produced narrower 95% CIs. However, since the model is misspecified (its assumptions are not met with the data provided), its results are biased, and the narrower 95% CIs are converging around a biased value (the MLR regression coefficient is a non-consistent estimator of the true Glucose-covariate relationships.) 

Note that this also explains why the probability of a Type I error is  higher for the Cholesterol regression coefficient (increases to >75% with n>10,000 samples) than it is for BP and BMI regression coefficients. The first plot (MLR coefficient estimates vs. samples size) showed us how  the MLR model produced especially biased estimates for the Cholesterol regression coefficient; therefore, as the 95% CI shrank towards a biased value with increasing sample size, the risk of a Type I error grew faster for this regression coefficient in particular. 


### b. The effect of outcome & covariate variable skewness on bias and inference, using a model which assumes *iid* and normally distributed data

- Simulation procedure: 
  - Vary through 7 degrees of freedom settings, applied to all four t-distributions being generated/modeled
    - For each sample size, extract...
      - Estimated regression coefficient for each covariate (Cholesterol, BMI, BP) 
      - Whether or not the "normal" 95% CI included the true theoretical value for the regression coefficient for each covariate (Cholesterol, BMI, BP)
  - For each df setting: Simulate (repeat the above) 1000 times
    - For each covariate, we will summarize...
      - The mean estimated regression coefficient (across runs) & empirical 95% CI – *This gives us insight into the bias and variability of estimates for models with data from each degrees-of-freedom setting for the t-distributions, which dictates skewness & how much the t-distribution approaches a normal distribution.*
      - The percentage of "normal" 95% CIs (% of runs) which included the true value – *This gives us insight into the probability of a Type I error, where the confidence interval generated by the the multiple linear regression does not include the true theoretical value. We asses how this changes with each degrees-of-freedom setting for the t-distributions, which dictates skewness & how much the t-distribution approaches a normal distribution.*
  
```{r}
# Vary skewness
# 10 different sample sizes -- explore 10 different orders of magnitude of sample size (rounded)
vary_skewness <- 3^seq(1,7)

# generate samples & store correlations
sampsize_gen = 500 # arbitrary choice -- generated sample size fixed for these simulations

# custom fxn
# function: store beta & 95% CI (calculated assuming linearity) from each regression--varying samp size
lm_varyskew_storebetas <- function(fxn_gen, samp_size, tdist_degfree){ # give a fxn to generate data, list of degrees of freedom to iterate through, & sample size for generated sample (fixed in this simulation)
  
  # store lm outputs in a list
  lm_out <- list()
  
  # iterate through different sample sizes [# samples generated from fxn]
  for (skew in tdist_degfree){
    
    gendata <- fxn_gen(n = samp_size, nu = skew)
    # run regression
    lm_nn <- lm(gendata$Glucose ~ gendata$Cholesterol + gendata$BMI + gendata$BloodPressure)
    
    # reformat to make it easy to combine/compress later for plotting -- each list entry is a dataframe
    # CIs
    lm_outputs <- as.data.frame(confint(lm_nn))
    colnames(lm_outputs) <- c("lower_95", "upper_95")
    lm_outputs <- lm_outputs[2:4,]
    rownames(lm_outputs) <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    # point values
    lm_outputs$point_val <- lm_nn$coefficients[2:4] # note: this works VERY specifically as written for our scenario--subsets are hard-coded for simplicity
    # marker for loop [skewness] & true values & pairname
    lm_outputs$tdist_df <- skew
    lm_outputs$true_val <- c(0.3, 0, 0) # from target_reg_coeffs, above
    lm_outputs$regcoeff <- c("Gluc_Chol", "Gluc_BMI", "Gluc_BP")
    
    # store in list
    lm_out[[paste0("tdistdf", skew)]] <- lm_outputs
  }
  
  # exit loop, return filled-in list    
  return(lm_out)
}
```

```{r}
# Now: repeat this 1000 times -- to get the % of CIs which cover the true value [in those 1000 replications/simulations] [for a given sample size]

# note: no need to differentiate between the replications, so they can be stored without numbering

num_simulations = 1000 # do 1000 runs

run_simstudy1_varyskewness <- function(nsim){
  # store each run of the simulation
  list_dfs <- list()
  # run #nsim times
  for (ii in seq(1,nsim)){
    simstudy1_listdfs <- lm_varyskew_storebetas(fxn_gen = generate_t_copula_data, 
                                                samp_size = sampsize_gen, tdist_degfree = vary_skewness)
    simstudy1_df <- do.call(rbind, simstudy1_listdfs) # note: can scale this to cover a wider range of sample sizes
    rownames(simstudy1_df) <- seq(1,nrow(simstudy1_df))
    list_dfs[[ii]] <- simstudy1_df
  }
  # unlist/combine, clean up -- no need to identify iterations
  simstudy1_nsim <- do.call(rbind, list_dfs) # note: can scale this to cover a wider range of sample sizes
  rownames(simstudy1_nsim) <- seq(1,nrow(simstudy1_nsim))
  return(simstudy1_nsim)
}
```

```{r, include = FALSE}
# generate 1000 samples
set.seed(445578)

# COMMENTED OUT (saved results to csv: takes several minutes to run)

# simstudy1_1000sim_1 <- run_simstudy1_varyskewness(num_simulations)
# 
# # save output (takes time to run)
# write.csv(simstudy1_1000sim_1, "simstudy1_1000sim_1.csv", row.names = FALSE)
```

```{r, include=FALSE}
# read in csv
simstudy1_1000sim_1 <- read.csv(here("./simstudy_1/simstudy1_1000sim_1.csv"))

# create confidence interval coverage statistics
library(dplyr)
library(tidyr)

# generate boolean variable -- was the true value contained within the 95% CI
simstudy1_1000sim_1$trueval_inCI <- (simstudy1_1000sim_1$true_val <= simstudy1_1000sim_1$upper_95) &
  (simstudy1_1000sim_1$true_val >= simstudy1_1000sim_1$lower_95)

# get the percentage for which this was true among the [num_simulations] simulations FOR each reg coeff (3) FOR each [sampsize] setting
simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI))

# check
# simstudy1_1000sim[simstudy1_1000sim$regcoeff=="Gluc_BP" & simstudy1_1000sim$gensampsize == 147, ] # 3 false, matches up! : )

# get empirical 95% CI & mean based off the [num_simulations] values/replications


simstudy1_1000sim_1_typeone <- simstudy1_1000sim_1 |> 
  group_by(tdist_df, regcoeff) |> 
  summarise(in_CI_pct = mean(trueval_inCI),
            empirical_lower95 = quantile(point_val, 0.05),   # empirical 95% CI and mean estimated betas from [num_simulations]
            empirical_upper95 = quantile(point_val, 0.95),
            empirical_mean = quantile(point_val, 0.5),
            true_val = mean(true_val))
```


```{r, echo=FALSE}
# Plot a) empirical CIs results against the theoretical values (visualize precison, bias)

ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = empirical_mean)) +
  
  geom_point() +
  geom_errorbar(aes(ymin = empirical_lower95, ymax = empirical_upper95), 
                width = 0.2, position = position_dodge(width = 0.9)) + 
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  # plot theoretical value
  geom_abline(data = simstudy1_1000sim_1_typeone, aes(intercept=true_val, slope = 0), color="green", size=0.3)+
  ggtitle("MLR coefficient estimates vs. t-dist degrees of freedom ") + 
    labs(x = "df of t-distributions (for Glucose, BMI, BP, & Cholesterol)",
         y = "Mean reg. coeff. estimate across runs (empirical 95% CI)",
         subtitle = "n = 1000 runs. Green line indicates true value.",
         caption = "With increasing df, the t-distributions of the outcome and covariates approach normality, 
         and bias in the MLR estimates decreases.")

# Plot b) % "in CI" (visualize probability of Type I error)
ggplot(data=simstudy1_1000sim_1_typeone, aes(x = tdist_df, y = in_CI_pct)) +
  geom_line(col="gray") + 
  geom_point() +
  facet_wrap(~regcoeff, scales="free_y",
             labeller = labeller(regcoeff = coeff_names)) + 
  scale_x_log10() +
  ggtitle("1 - P(Type I Error) vs. t-dist degrees of freedom ") + 
  labs(x = "df of t-distributions (for Glucose, BMI, BP, & Cholesterol)",
       y = "Proportion of runs where MLR 95% CIs included true value",
       subtitle = "n = 1000 runs.",
       caption = "With higher df, t-distribution approaches normal distribution. Increasing the degrees of freedom 
       the data-generating mechanisms led to less incorrect inference; as the outcome and covariate distributions
       approached normality, the model's assumptions were less and less violated (less bias).")

```


_Interpretation:_

As the degrees of freedom to the t-distributions increased increased, the bias of the model-estimated regression coefficients decreased. This is as we would expect. The t-distribution approaches the normal distribution as the degrees of freedom approaches infinity; therefore, as we increased the degrees of freedom, we lessened the violation of the MLR that we had introduced by using skewed, non-normal distributions in the data. (Note: we see that precision was not affected by the reduction in skewness. This is as expected, given that sample size was fixed in these simulations.)

As the degrees of freedom increased (i.e. less deviation from normality), the probability of the MLR 95% CIs *including* the true regression coefficient values increased, indicating that the probability of those CIs *excluding* the true regression coefficient values (i.e. the probability of a Type I error) decreased. This trend was clear for all three covariates. In fact, all three covariates converged around a 95% probability of including the true coefficient value, meaning a 5% probability of a Type I error. This indicates that, as the t-distributions approached normality, the 95% CIs generated by MLR (under the assumption of normality) converged to being _true_ 95% CIs.


# 2. Simulation Study 2

## Design Simulation study 2

Identify a statistical inference tool that could accommodate the correlation and non-normality ((e.g., a robust regression approach, generalized estimating equations (GEE) for correlated data, or a Bayesian method with appropriate priors).

Use simulation that is similar to Simulation 1 to this inference tool provides valid results for non-normal, correlated data, and compare these results to the standard inference tool in study 1.

For this simulation, robust regression was used to mitigate the influence of outliers and heavy-tailed distributions. It is useful when ordinary least squares assumptions don't hold up Specifically, MM-estimation was performed because this method is useful when handeling correlated and non-normal data. The model works by using two steps. The first step is an initial robust estimation with $\beta$ of high resistance to outliers, and the second step involves refining the estimate using a robust lost function. This form of regression downweigns the influence of outliers and works well with skewed data. MM-estimation robust regressing has a high breakdown point compared to other regression models, meaning a high fraction of outliers for which a model won't break down.
```{r}
library(tidyverse)
library(ggplot2)
library(robustbase)
library(copula)
library(Matrix)
```

```{r}
#taken from parts 1 and 2 and turned into a function so I can change parapeters in the simulation

#function to generate correlated non-normal data using a t-Copula (used part 2)
generate_t_copula_data <- function(n, nu, rho_mod = NULL) {
  #block-diagonal correlation structure
  block1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)  #metabolic block
  block2 <- matrix(c(1, 0.4, 0.4, 1), 2, 2)  #cardiovascular block
  
  #modify correlation for later
  if (!is.null(rho_mod)) {
    block1[1,2] <- block1[2,1] <- rho_mod[1]
    block2[1,2] <- block2[2,1] <- rho_mod[2]
  }
  
  #correlation matrix
  Sigma <- bdiag(block1, block2)
  Sigma <- as.matrix(nearPD(Sigma)$mat)
  rho_vec <- Sigma[lower.tri(Sigma)]  
  
  #define the t-Copula
  copula_t <- tCopula(param=rho_vec, dim=4, df=nu, dispstr="un")
  U_t <- rCopula(n, copula_t)  
  
  #generate independent heavy-tailed distributions
  mu <- c(100, 200, 25, 120) #the means
  
# Define upper and lower bounds for each variable
glucose_min <- 32  # minimum plausible fasting glucose level in mg/dL
glucose_max <- 500 # maximum plausible fasting glucose level in mg/dL

cholesterol_min <- 100  # minimum plausible cholesterol level in mg/dL
cholesterol_max <- 300  # maximum plausible cholesterol level in mg/dL

BMI_min <- 15    # minimum plausible BMI
BMI_max <- 40    # maximum plausible BMI

blood_pressure_min <- 90   # minimum plausible systolic blood pressure in mmHg
blood_pressure_max <- 180  # maximum plausible systolic blood pressure in mmHg

  glucose_raw <- scale(rt(10000, df=nu)) * 15 + mu[1]
  cholesterol_raw <- scale(rt(10000, df=nu)) * 25 + mu[2]
  bmi_raw <- scale(rt(10000, df=nu)) * 3 + mu[3]
  blood_pressure_raw <- scale(rt(10000, df=nu)) * 10 + mu[4]
  
  # Filter out values that fall outside the realistic human limits
glucose <- glucose_raw[glucose_raw >= glucose_min & glucose_raw <= glucose_max]
cholesterol <- cholesterol_raw[cholesterol_raw >= cholesterol_min & cholesterol_raw <= cholesterol_max]
bmi <- bmi_raw[bmi_raw >= BMI_min & bmi_raw <= BMI_max]
blood_pressure <- blood_pressure_raw[blood_pressure_raw >= blood_pressure_min & blood_pressure_raw <= blood_pressure_max]

  #map Copula Uniform Samples Back to Heavy-Tailed Distributions
  data_copula_t <- data.frame(
    glucose = quantile(glucose, probs = U_t[,1]),
    cholesterol = quantile(cholesterol, probs = U_t[,2]),
    bmi = quantile(bmi, probs = U_t[,3]),
    blood_pressure = quantile(blood_pressure, probs = U_t[,4])
  )
  return(data_copula_t)
}

```
$$
\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n} \rho \left( \frac{y_i - x_i^\top \beta}{\hat{\sigma}} \right)
$$
$y_i$ : dependent variable
$x_i$ : predictor variable
$\beta$ : regression coefficients
$\sigma$ : robust estimate of residual standard deviation
$\rho$: Robust loss function

Original Data & Robust Regression
```{r}
set.seed(2025)

original_data <- generate_t_copula_data(n = 100, nu = 5)
original_data <- original_data %>%
  mutate(across(everything(), ~ . - mean(.)))#centering the data. this prevents the intercept variance from over affecting the data 
original_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = original_data)
summary(original_model) #robust regression using MM
```
Larger Sample Size

```{r}
set.seed(2025)

large_data <- generate_t_copula_data(n = 500, nu = 5)
large_data <- large_data %>%
  mutate(across(everything(), ~ . - mean(.)))
large_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = large_data)
summary(large_model)
```
Varying Correlation Levels

```{r}
set.seed(2025)
high_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.8, 0.7))
high_corr_data <- high_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_corr_data)
summary(high_corr_model)

low_corr_data <- generate_t_copula_data(n = 100, nu = 5, rho_mod = c(0.2, 0.1))
low_corr_data <- low_corr_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_corr_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_corr_data)
summary(low_corr_model)
```

Varying Non-Normality Degree
```{r}
low_df_data <- generate_t_copula_data(n = 100, nu = 4)
low_df_data <-low_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
low_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = low_df_data)
summary(low_df_model)

high_df_data <- generate_t_copula_data(n = 100, nu = 6)
high_df_data <- high_df_data %>%
  mutate(across(everything(), ~ . - mean(.)))
high_df_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = high_df_data)
summary(high_df_model)

```
Lower than 4, and the data did not converge
```{r}
set.seed(2025)
bmi_data <- generate_t_copula_data(n = 100, nu = 5)

# Add a strong linear effect of bmi on glucose
bmi_data$glucose <- bmi_data$glucose + 2 * bmi_data$bmi  # Adjust the coefficient (0.5) as needed

#center
bmi_data <- bmi_data %>% mutate(across(everything(), ~ . - mean(.)))

#robust regression model
bmi_model <- lmrob(glucose ~ blood_pressure + cholesterol + bmi, data = bmi_data)
summary(bmi_model)
```

Visualizing results

```{r}

true_mean_glucose <- 0 #because data was centered around 0

#variance
calculate_variance <- function(model) {
  return(diag(vcov(model)))
}

#bias for glucose 
calculate_bias_glucose <- function(model, true_mean_glucose, data) {
  predicted_mean_glucose <- predict(model, newdata = data.frame(
    cholesterol = 0,  # all centered
    bmi = 0,          
    blood_pressure = 0
  ))
  
  #bias = predicted mean - true mean
  bias <- predicted_mean_glucose - true_mean_glucose
  return(bias)
}

#store results
store_results <- function(model, scenario_name, true_mean_glucose, data) {
  variance <- calculate_variance(model)
  bias <- calculate_bias_glucose(model, true_mean_glucose, data)
  
  data.frame(
    Scenario = scenario_name,
    Variance_Intercept = variance[1],
    Variance_BloodPressure = variance[2],
    Variance_Cholesterol = variance[3],
    Variance_BMI = variance[4],
    Bias_Glucose = bias
  )
}

#store results for each scenario
results <- rbind(
  store_results(original_model, "Original Data", true_mean_glucose, original_data),
  store_results(large_model, "Larger Sample Size", true_mean_glucose, large_data),
  store_results(high_corr_model, "High Correlation Levels", true_mean_glucose, high_corr_data),
  store_results(low_corr_model, "Low Correlation Levels", true_mean_glucose, low_corr_data),
  store_results(high_df_model, "High df (Less Non-Normal)", true_mean_glucose, high_df_data),
  store_results(low_df_model, "Low df (More Non-Normal)", true_mean_glucose, low_df_data),
  store_results(bmi_model, "BMI Effect Model", true_mean_glucose, bmi_data)
)

#results table
kable(results, caption = "Comparison of Variance and Bias Across Different Scenarios")
```


```{r}
#variance (for coefficients)
variance_long <- results %>%
  pivot_longer(cols = starts_with("Variance"), 
               names_to = "Coefficient", 
               values_to = "Variance")

ggplot(variance_long, aes(x = Scenario, y = Variance, fill = Coefficient)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Variance of Coefficients", x = "Scenario", y = "Variance") +
  theme_minimal()
```


```{r}
#bias (for glucose)
ggplot(results, aes(x = Scenario, y = Bias_Glucose, fill = Scenario)) +
  geom_bar(stat = "identity") +
  labs(title = "Bias of Glucose Predictions", x = "Scenario", y = "Bias") +
  theme_minimal()
```

```{r}
library(dplyr)
library(knitr)

# Function to extract R^2 and significant predictors
extract_results <- function(model, scenario_name) {
  # Extract R^2
  r_squared <- summary(model)$r.squared
  
  # Extract coefficients and p-values
  coef_table <- summary(model)$coefficients
  significant_predictors <- rownames(coef_table)[coef_table[, "Pr(>|t|)"] < 0.05]
  
  # Remove intercept from significant predictors
  significant_predictors <- significant_predictors[significant_predictors != "(Intercept)"]
  
  # Combine into a data frame
  data.frame(
    Scenario = scenario_name,
    R_Squared = r_squared,
    Significant_Predictors = paste(significant_predictors, collapse = ", ")
  )
}

results_table <- rbind(
  extract_results(original_model, "Original Data"),
  extract_results(large_model, "Larger Sample Size"),
  extract_results(high_corr_model, "High Correlation Levels"),
  extract_results(low_corr_model, "Low Correlation Levels"),
  extract_results(high_df_model, "High df (Less Non-Normal)"),
  extract_results(low_df_model, "Low df (More Non-Normal)"),
  extract_results(bmi_model, "BMI Effect")
)


kable(results_table, caption = "Model Performance and Significant Predictors")
```


# Discussion
# Key Takeaways
*fill this in from the slides*

# Appendix: Full Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

## References

http://dx.doi.org/10.12732/ijpam.v91i3.7
